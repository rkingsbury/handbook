{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Persson Group Handbook This website serves as the group handbook for the Persson Group at the University of California, Berkeley / Lawrence Berkeley National Laboratory. The purpose of this handbook is to provide information to new students and post-docs who do not yet have full access to internal group resources when they first join the group (hence why this handbook is publically available). This handbook is just the starting point to ensure a smoother integration of new group members and an efficient onboarding process. Our newest group members often have the most valuable insight in improving the onboarding process and where there may be critical gaps in the handbook. While the Onboarding Guides have ultimate responsibility of overseeing the handbook (see group jobs ), all Persson Group members are encouraged to contribute to this handbook via the open-source repo.","title":"The Persson Group Handbook"},{"location":"#the-persson-group-handbook","text":"This website serves as the group handbook for the Persson Group at the University of California, Berkeley / Lawrence Berkeley National Laboratory. The purpose of this handbook is to provide information to new students and post-docs who do not yet have full access to internal group resources when they first join the group (hence why this handbook is publically available). This handbook is just the starting point to ensure a smoother integration of new group members and an efficient onboarding process. Our newest group members often have the most valuable insight in improving the onboarding process and where there may be critical gaps in the handbook. While the Onboarding Guides have ultimate responsibility of overseeing the handbook (see group jobs ), all Persson Group members are encouraged to contribute to this handbook via the open-source repo.","title":"The Persson Group Handbook"},{"location":"about/about/","text":"About the Group Our group studies the physics and chemistry of materials using atomistic computational methods and high-performance computing technology. Most of our research is focused on materials for energy applications, such as battery electrode materials, electrolytes, photocatalysts, thermoelectrics, etc. We are co-located at Lawrence Berkeley National Laboratory (LBNL) and the University of California, Berkeley, just down the hill from LBNL. Our main workspaces are located at LBNL, but we have a number of additional desks available on campus for students to use in between classes, which are located in the grad bay of Hearst Memorial Mining Building. San Francisco is located across the bay from Berkeley and is about a 30 minute drive or BART train ride away. Berkeley itself is a vibrant city of 115,000 people filled with cafes, restaurants of all types, and cultural activities. Group events Some of the regular things we do as a group are: Lunch every two weeks jointly with our group and members of the Jain research group. A group outing once every semester. Past activities included paintball and cardboard boat races in Kristin's pool. Twice-yearly \u201cCodeBusters\u201d events, where we spend 3 days hacking on code together. Dinners to celebrate group members who are moving on to join universities or companies. An annual joint holiday party with the Ceder Group.","title":"About"},{"location":"about/about/#about-the-group","text":"Our group studies the physics and chemistry of materials using atomistic computational methods and high-performance computing technology. Most of our research is focused on materials for energy applications, such as battery electrode materials, electrolytes, photocatalysts, thermoelectrics, etc. We are co-located at Lawrence Berkeley National Laboratory (LBNL) and the University of California, Berkeley, just down the hill from LBNL. Our main workspaces are located at LBNL, but we have a number of additional desks available on campus for students to use in between classes, which are located in the grad bay of Hearst Memorial Mining Building. San Francisco is located across the bay from Berkeley and is about a 30 minute drive or BART train ride away. Berkeley itself is a vibrant city of 115,000 people filled with cafes, restaurants of all types, and cultural activities.","title":"About the Group "},{"location":"about/about/#group-events","text":"Some of the regular things we do as a group are: Lunch every two weeks jointly with our group and members of the Jain research group. A group outing once every semester. Past activities included paintball and cardboard boat races in Kristin's pool. Twice-yearly \u201cCodeBusters\u201d events, where we spend 3 days hacking on code together. Dinners to celebrate group members who are moving on to join universities or companies. An annual joint holiday party with the Ceder Group.","title":"Group events"},{"location":"about/acknowlegements/","text":"A large part of the material in the 'Getting Started' guide was adopted from the Jain Group Handbook , written by Anubhav Jain.","title":"Acknowlegements"},{"location":"about/group_jobs/","text":"Persson Group Jobs - Spring 2021 (Last updated Novemeber 30, 2020) Administrative Job Name Group Manager: Assign desk spaces at LBL and on campus, keep track of laptops (LBL/Berkeley) and large assets with a DOE tag Rebecca Stern Job Master: Assign all group jobs, create/remove/transfer jobs as needed, manage job transitions between new group members and alumni Matt McDermott Onboarding Guide (Grads): Assist new graduate students in getting started at LBL, create & manage onboarding resources and welcome events Martin Siron Onboarding Guide (Postdocs/Staff): Assist new postdocs/staff in getting started at LBL, create & manage onboarding resources and welcome events, add new members to Google Group Ryan Kingsbury Postdoc HR: Coordinate interview process for new postdocs, programming challenge evaluation Jianli Cheng, Sam Blau Seminar Master: Schedule and coordinate group talks/seminars Trevor Seguin Undergraduate Coordinator: Facilitate connection between undergraduates and available projects within the group Jiyoon Kim Infrastructure Job Name Group Webmaster: Update the Persson Group public website Guy Moore, Evan Spotte-Smith Internal Site Manager: Manage internal group site and its resources David Mrdjenovich MP Swag Master: Coordinate the design and ordering of MP swag, including t-shirts, mugs, etc. Ann Rutt MP Wiki Master: Manage Materials Project Docs and MP presence on Wikipedia Tingzheng Hou Production Gatekeeper: Assist with production jobs and group data management Patrick Huck Publications Manager: Assist Alice in publications for reports Xiaowei Xie Computing Job Name DFT + Atomate/Pymatgen Support: Assist with creating DFT-based workflows, manage VASP licenses, and answer questions about pymatgen Jason Munro, Jimmy Shen, Matt Horton Emmet Guru: Manage database builders; train new people in builders Shyam Dwaraknath HPC Resource Manager: Apply for time on NERSC and NREL, distribute and manage NERSC time, VASP compilation on Savio, Lawrencium, and Eagle Eric Sivonxay Matsci.org Moderator: Manage Q&A about Materials Project on matsci.org Maxwell Venetos MD Wizard: Assist with setup and running of molecular dynamics simulations Kara Fong Molecular Explorer: Assist with QChem workflows and compiling support Sam Blau Extracurriculars Job Name *Coffee Tsar: Supply coffee for espresso machine Ruoxi Yang *Group Chef: Prepare snacks/food for group meetings Jordan Burns, Caitlin McCandler, Jimmy Shen *Group Historian: Schedule and take group picture at least twice a year; document group outings/events and maintain these on the shared drive Oxana Andriuc Kavli Representative: Attend Kavli ENSI meetings once per month, prepare materials for distribution/presentation at Kavli events, facilitate collaborations between Kavli member groups, assist students in applying to Kavli awards and fellowships Caitlin McCandler Outreach Officer: Organize and disseminate videos from MP workshop; help create exportable MP content for the community Rachel Woods-Robinson *PrinterMan: Outline directions for printing and update printer regularly with supplies Julian Self Social Chair: *Plan major group outings and retreat; assist in planning other socials Alex Epstein, Handong Ling Social Media Chair: Represent the group on Twitter/Facebook; assist in outreach John Dagdelen * Indicates job duties that are paused during COVID-19 pandemic.","title":"Group Jobs"},{"location":"about/group_jobs/#persson-group-jobs-spring-2021","text":"(Last updated Novemeber 30, 2020)","title":"Persson Group Jobs - Spring 2021"},{"location":"about/group_jobs/#administrative","text":"Job Name Group Manager: Assign desk spaces at LBL and on campus, keep track of laptops (LBL/Berkeley) and large assets with a DOE tag Rebecca Stern Job Master: Assign all group jobs, create/remove/transfer jobs as needed, manage job transitions between new group members and alumni Matt McDermott Onboarding Guide (Grads): Assist new graduate students in getting started at LBL, create & manage onboarding resources and welcome events Martin Siron Onboarding Guide (Postdocs/Staff): Assist new postdocs/staff in getting started at LBL, create & manage onboarding resources and welcome events, add new members to Google Group Ryan Kingsbury Postdoc HR: Coordinate interview process for new postdocs, programming challenge evaluation Jianli Cheng, Sam Blau Seminar Master: Schedule and coordinate group talks/seminars Trevor Seguin Undergraduate Coordinator: Facilitate connection between undergraduates and available projects within the group Jiyoon Kim","title":"Administrative"},{"location":"about/group_jobs/#infrastructure","text":"Job Name Group Webmaster: Update the Persson Group public website Guy Moore, Evan Spotte-Smith Internal Site Manager: Manage internal group site and its resources David Mrdjenovich MP Swag Master: Coordinate the design and ordering of MP swag, including t-shirts, mugs, etc. Ann Rutt MP Wiki Master: Manage Materials Project Docs and MP presence on Wikipedia Tingzheng Hou Production Gatekeeper: Assist with production jobs and group data management Patrick Huck Publications Manager: Assist Alice in publications for reports Xiaowei Xie","title":"Infrastructure"},{"location":"about/group_jobs/#computing","text":"Job Name DFT + Atomate/Pymatgen Support: Assist with creating DFT-based workflows, manage VASP licenses, and answer questions about pymatgen Jason Munro, Jimmy Shen, Matt Horton Emmet Guru: Manage database builders; train new people in builders Shyam Dwaraknath HPC Resource Manager: Apply for time on NERSC and NREL, distribute and manage NERSC time, VASP compilation on Savio, Lawrencium, and Eagle Eric Sivonxay Matsci.org Moderator: Manage Q&A about Materials Project on matsci.org Maxwell Venetos MD Wizard: Assist with setup and running of molecular dynamics simulations Kara Fong Molecular Explorer: Assist with QChem workflows and compiling support Sam Blau","title":"Computing"},{"location":"about/group_jobs/#extracurriculars","text":"Job Name *Coffee Tsar: Supply coffee for espresso machine Ruoxi Yang *Group Chef: Prepare snacks/food for group meetings Jordan Burns, Caitlin McCandler, Jimmy Shen *Group Historian: Schedule and take group picture at least twice a year; document group outings/events and maintain these on the shared drive Oxana Andriuc Kavli Representative: Attend Kavli ENSI meetings once per month, prepare materials for distribution/presentation at Kavli events, facilitate collaborations between Kavli member groups, assist students in applying to Kavli awards and fellowships Caitlin McCandler Outreach Officer: Organize and disseminate videos from MP workshop; help create exportable MP content for the community Rachel Woods-Robinson *PrinterMan: Outline directions for printing and update printer regularly with supplies Julian Self Social Chair: *Plan major group outings and retreat; assist in planning other socials Alex Epstein, Handong Ling Social Media Chair: Represent the group on Twitter/Facebook; assist in outreach John Dagdelen * Indicates job duties that are paused during COVID-19 pandemic.","title":"Extracurriculars"},{"location":"about/group_resources/","text":"Printing and Scanning The group has a Canon MF731C printer located in 33-143C. It supports color printing as well as two-sided printing and includes a scanner. To connect to the printer follow this guide on the internal group site .","title":"Group Resources"},{"location":"about/group_resources/#printing-and-scanning","text":"The group has a Canon MF731C printer located in 33-143C. It supports color printing as well as two-sided printing and includes a scanner. To connect to the printer follow this guide on the internal group site .","title":"Printing and Scanning "},{"location":"about/roster/","text":"Group Roster The group roster is available on the group's Google Drive .","title":"Group Roster"},{"location":"about/roster/#group-roster","text":"The group roster is available on the group's Google Drive .","title":"Group Roster "},{"location":"computing/ab_initio/","text":"VASP We generally run 2 versions of VASP - 5.4.4 and 6 (in beta). VASP 6 is used primarily on nodes operating on the Knights Landing (KNL) CPU architecture, while VASP 5.4.4 is generally used on all other CPUs. VASP on NERSC NERSC maintains compiled, and optimized versions of the vasp binaries. To get access to these binaries, you must be added to the group VASP license. If/when you determine that your research requires the use of VASP, contact Jimmy Shen and Eric Sivonxay. The potcar directory is: /project/projectdirs/matgen/POTCARs VASP on Berkeley Research Computing (savio) When running vasp on savio, we have access to our own compilations. To access the vasp binaries on savio, add the following line to your .bashrc file. export MODULEPATH=${MODULEPATH}:/global/home/groups/co_lsdi/sl7/modfiles POTCARs are located at: /clusterfs/cloudcuckoo/POTCARs For issues with these compilations, please contact Eric Sivonxay (esivonxay@lbl.gov) Lawrencium When running vasp on Lawrencium, we have access to our own compilations. To access the vasp binaries on Lawrencium, add the following line to your .bashrc file. export MODULEPATH=${MODULEPATH}:/clusterfs/mp/temp_modfiles POTCARs are located at: /clusterfs/mp/software/POTCARs For issues with these compilations, please contact Eric Sivonxay (esivonxay@lbl.gov) Q-Chem More Information to come Authors: Eric Sivonxay Contact: esivonxay@lbl.gov","title":"Ab Initio"},{"location":"computing/ab_initio/#vasp","text":"We generally run 2 versions of VASP - 5.4.4 and 6 (in beta). VASP 6 is used primarily on nodes operating on the Knights Landing (KNL) CPU architecture, while VASP 5.4.4 is generally used on all other CPUs.","title":"VASP"},{"location":"computing/ab_initio/#vasp-on-nersc","text":"NERSC maintains compiled, and optimized versions of the vasp binaries. To get access to these binaries, you must be added to the group VASP license. If/when you determine that your research requires the use of VASP, contact Jimmy Shen and Eric Sivonxay. The potcar directory is: /project/projectdirs/matgen/POTCARs","title":"VASP on NERSC"},{"location":"computing/ab_initio/#vasp-on-berkeley-research-computing-savio","text":"When running vasp on savio, we have access to our own compilations. To access the vasp binaries on savio, add the following line to your .bashrc file. export MODULEPATH=${MODULEPATH}:/global/home/groups/co_lsdi/sl7/modfiles POTCARs are located at: /clusterfs/cloudcuckoo/POTCARs For issues with these compilations, please contact Eric Sivonxay (esivonxay@lbl.gov)","title":"VASP on Berkeley Research Computing (savio)"},{"location":"computing/ab_initio/#lawrencium","text":"When running vasp on Lawrencium, we have access to our own compilations. To access the vasp binaries on Lawrencium, add the following line to your .bashrc file. export MODULEPATH=${MODULEPATH}:/clusterfs/mp/temp_modfiles POTCARs are located at: /clusterfs/mp/software/POTCARs For issues with these compilations, please contact Eric Sivonxay (esivonxay@lbl.gov)","title":"Lawrencium"},{"location":"computing/ab_initio/#q-chem","text":"More Information to come Authors: Eric Sivonxay Contact: esivonxay@lbl.gov","title":"Q-Chem"},{"location":"computing/atomate/","text":"Setup an environment: Create the conda environment Nersc module load python/3.6-anaconda-4.4 conda create -n cms python=3.6 Savio module load python/3.6 conda create -n cms python=3.6 Lawrencium module load python/3.6 conda create -n cms python=3.6 Activate the environment and install the base libraries Note: cms=computational materials science, but feel free to pick your own name source activate cms conda install numpy scipy matplotlib pandas to install atomate on the environment, use one of the following: If you are just using atomate and fireworks, with no plans to develop code/ workflows, use the following: pip install atomate For developers: source activate cms cd ~/.conda/envs/cms mkdir code cd code git clone https://github.com/materialsproject/pymatgen.git git clone https://github.com/materialsproject/pymatgen-db.git git clone https://github.com/materialsproject/fireworks.git git clone https://github.com/materialsproject/custodian.git git clone https://github.com/hackingmaterials/atomate.git cd pymatgen python setup.py develop pip install -r requirements.txt cd .. cd pymatgen-db python setup.py develop pip install -r requirements.txt cd .. cd fireworks python setup.py develop pip install -r requirements.txt cd .. cd custodian python setup.py develop pip install -r requirements.txt cd .. cd atomate python setup.py develop pip install -r requirements.txt cd .. Configure Fireworks: Create Fireworks configuration files Make directory in environment\u2019s root directory (/envs/<name>/) called config cd ~/.conda/envs/cms mkdir config cd config Make 3 files: FW_config.yaml, db.json, and my_launchpad.yaml with the following contents. Replace the teal highlighted text with details specific to your configuration. Note that you can view your filesystem online . Fireworks configuration (FW_config.yaml) Nersc CONFIG_FILE_DIR: /global/homes/s/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Savio CONFIG_FILE_DIR: /global/home/users/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Lawrencium CONFIG_FILE_DIR: /global/home/users/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Database file (db.json) { \"host\": \"mongodb03.NERSC.gov\", \"port\": 27017, \"database\": \"fw_db_name\", \"collection\": \"tasks\", \"admin_password\": \"<admin_password>\", \"admin_user\": \"<admin_username>\", \"readonly_password\": \"<readonly_password>\", \"readonly_user\": \"<readonly_username>\", \"aliases_config\": null } Fireworks LaunchPad file (my_launchpad.yaml) host: mongodb03.NERSC.gov port: 27017 name: 'fw_db_name' username: '<admin_username>' password: \u2018<admin_password>\u2019 logdir: null Istrm_lvl: DEBUG user_indices: [] wf_user_indices: [] Create Fireworker and Queue Launchers: Fireworker file (my_fworker.yaml) Nersc name: NERSC_fworker category: '' query: '{}' env: db_file: /global/homes/s/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'srun -n 64 -c 4 --cpu_bind=cores vasp_std' gamma_vasp_cmd: 'srun -n 64 -c 4 --cpu_bind=cores vasp_gam' scratch_dir: /global/cscratch1/sd/sivonxay incar_update: Savio name: savio_fworker category: '' query: '{}' env: db_file: /global/home/users/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'mpirun --bind-to core vasp_std' gamma_vasp_cmd: 'mpirun --bind-to core vasp_gam' scratch_dir: /global/scratch/sivonxay/ incar_update: Lawrencium name: lrc_fworker category: '' query: '{}' env: db_file: /global/home/users/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'mpirun --bind-to core vasp_std' gamma_vasp_cmd: 'mpirun --bind-to core vasp_gam' scratch_dir: /global/scratch/sivonxay/ incar_update: Queue Adapter (my_qadapter.yaml): Nersc _fw_name: CommonAdapter _fw_q_type: SLURM rocket_launch: rlaunch -w /global/homes/s/sivonxay/.conda/envs/cms/config/my_fworker.yaml singleshot nodes: 1 walltime: '24:00:00' account: matgen job_name: knl_launcher signal: SIGINT@60 qos: regular constraint: 'knl' pre_rocket: | source activate cms module load vasp-tpc/5.4.4-knl export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 Savio _fw_name: CommonAdapter _fw_q_type: SLURM rocket_launch: rlaunch -w /global/home/users/sivonxay/.conda/envs/cms/config/my_fworker.yaml singleshot nodes: 1 walltime: '24:00:00' account: co_lsdi job_name: knl_launcher queue: savio2_knl qos: lsdi_knl2_normal ntasks: 64 pre_rocket: | source activate cms module load vasp export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 post_rocket: null Lawrencium Available queues, partitions, and qos can be found at the following links: NERSC Savio Lawrencium Note: specifying singleshot in the queue adapter will limit each reserved job to running only one firework (even if other fireworks are ready and could run with your remaining wall time). Can change to rapidfire but this may result in lost runs (fireworks that do not complete because they run out of wall time). For more information on best practices for running VASP on multiple nodes (i.e. how to set vasp_cmd in my_fworker.yaml based on the number of nodes requested in my_qadapter) see the NERSC vasp training Choose the appropriate number of nodes, processes, and cores/process: Follow this flowchart to decide how many nodes to request, the number of parallel processes to use, and the number of cores to use for each parallel process. The rule of thumb to remember is that you should use all of the cores for every node that you request. For example, if there are 256 cores per node, you request N nodes, n processes, and c cores/process, then the following relationship must hold: 256 N = n c Specify N in my_qadapter.yaml and specify n and c in my_fworker.yaml within your configuration folder. Configure Bash profile: Append the following lines to the .bashrc.ext file (which is located in your home directory, e.g. /global/homes/s/sivonxay) Nersc module load python/3.6-anaconda-4.4 export PMG_VASP_PSP_DIR=/project/projectdirs/matgen/POTCARs export FW_CONFIG_FILE='/global/homes/s/sivonxay/.conda/envs/cms/config/FW_config.yaml' alias cdconfig='cd ~/.conda/envs/cms/config' Savio module load python/3.6 export PMG_VASP_PSP_DIR=/clusterfs/mp/software/POTCARs export FW_CONFIG_FILE='/global/home/users/sivonxay/.conda/envs/cms/config/FW_config.yaml' alias cdconfig='cd ~/.conda/envs/cms/config' Lawrencium Note the line alias cdconfig='cd ~/.conda/envs/cms/config' is optional but is recommended so you can more easily locate the directory with your configuration files stored by typing cdconfig into the command line. Setup API key and Add POTCAR Directory to pymatgen Go to materialsproject.org and get an API key. Make sure you are using the environment setup above to run these commands. Nersc pmg config --add PMG_MAPI_KEY <USER_API_KEY> pmg config --add PMG_VASP_PSP_DIR /project/projectdirs/matgen/POTCARs Savio pmg config --add PMG_MAPI_KEY <USER_API_KEY> pmg config --add PMG_VASP_PSP_DIR /clusterfs/cloudcuckoo/POTCARs Running these commands will create a .pmgrc.yaml file (if it doesn\u2019t exist already) containing these configuration settings in your home directory Using Fireworks and Atomate: Initializing Fireworks Database: Use care when running this command. It will wipe all existing entries in your fireworks database in the fireworks, workflows, and launches collections. lpad reset Using Atomate to create preset workflow The following code blocks are python code that should be run in jupyter or from the terminal. Make sure you are using the environment setup above. The computer running the code should have access to mongodb03.NERSC.gov; this can be disregarded when running directly on NERSC or when connected to to the LBNL intranet. For computers outside of LBNL, a VPN will need to be used. Run the following python code by creating a file named \u201cmake_workflow.py\u201d: from pymatgen import MPRester from fireworks import LaunchPad from atomate.vasp.workflows.presets.core import wf_static from atomate.vasp import powerups mpr = MPRester() structure = mpr.get_structure_by_material_id(\u201cmp-145\u201d) wf = wf_static(structure) wf = powerups.add_common_powerups(wf, {\"scratch_dir\":\">>scratch_dir<<\"}) lp = LaunchPad.auto_load() lp.add_wf(wf) The previous python script added a static energy calculation for Hexagonal Lu to your job database. python \u201cmake_workflow.py To check on the job, run the command: lpad get_fws -s READY Running Jobs: We recommend using a tmux or screen when submitting jobs to preserve queue submission when the main ssh session is terminated. This allows one to keep the queue saturated with jobs. For example, for tmux: module load tmux tmux new -s background_launcher source active cms mkdir FireworksTest cd FireworksTest To exit the tmux session, press the \u201ccontrol\u201d and \u201cb\u201d keys together, followed by \u201cd\u201d To re-enter the tmux session: tmux attach -t background_launcher For more tmux commands, see the tmux cheatsheet Use qlaunch command to reserve jobs with SLURM\u2019s scheduler. Qlaunch has 3 modes; singleshot, rapidfire, and multi. Singleshot is used to launch one job, rapidfire is used to launch multiple jobs in quick succession, and multi creates one job with multiple fireworks runs. You\u2019ll probably want to use rapidfire. Some useful flags to set are: -m to specify maximum # of jobs in queue at any given time and --nlaunches to specify how many fireworks to run. Here are some examples: Example_1 qlaunch singleshot Example_2 qlaunch rapidfire --nlaunches 10 Example_3 qlaunch rapidfire --nlaunches infinite -m 50 Example_4 qlaunch -fm rapidfire --nlaunches infinite -m 50 Monitoring Database/ Fireworks Status of running fireworks can be determined by using the command Example_1 lpad get_fws -s RUNNING Example_2 lpad get_fws -q '{\"state\":\"RUNNING\", \"fw_id\":{\"$gte\": 50}}' Rerunning fizzled (Failed) fireworks Example_1 lpad rerun_fws -i <fw_id> Example_2 lpad rerun_fws -s FIZZLED Alternatively, you can view your jobs in the GUI by downloading your my_launchpad.yaml file to your local machine. To do so: scp <username>@dtn01.NERSC.gov:~/.conda/envs/cms/config/my_launchpad.yaml . Then, run this command, locally, in the folder containing the my_launchpad.yaml file. This will bring up the web interface hosted on you local machine on port 5000 (127.0.0.1:5000). lpad webgui Occasionally, jobs may hang and not complete. To check for hung jobs, use the command: lpad detect_lostruns To rerun these jobs, add the --rerun flag to the command. lpad detect_lostruns --rerun Viewing Database and Outputs: Download and install Robo 3T from the following link Setup connection using mongodb credentials Querying collections Authors: Eric Sivonxay, Julian Self, Ann Rutt, and Oxana Andriuc Contact: esivonxay@lbl.gov and jself@lbl.gov","title":"Atomate Setup"},{"location":"computing/atomate/#setup-an-environment","text":"Create the conda environment Nersc module load python/3.6-anaconda-4.4 conda create -n cms python=3.6 Savio module load python/3.6 conda create -n cms python=3.6 Lawrencium module load python/3.6 conda create -n cms python=3.6 Activate the environment and install the base libraries Note: cms=computational materials science, but feel free to pick your own name source activate cms conda install numpy scipy matplotlib pandas to install atomate on the environment, use one of the following: If you are just using atomate and fireworks, with no plans to develop code/ workflows, use the following: pip install atomate For developers: source activate cms cd ~/.conda/envs/cms mkdir code cd code git clone https://github.com/materialsproject/pymatgen.git git clone https://github.com/materialsproject/pymatgen-db.git git clone https://github.com/materialsproject/fireworks.git git clone https://github.com/materialsproject/custodian.git git clone https://github.com/hackingmaterials/atomate.git cd pymatgen python setup.py develop pip install -r requirements.txt cd .. cd pymatgen-db python setup.py develop pip install -r requirements.txt cd .. cd fireworks python setup.py develop pip install -r requirements.txt cd .. cd custodian python setup.py develop pip install -r requirements.txt cd .. cd atomate python setup.py develop pip install -r requirements.txt cd ..","title":"Setup an environment:"},{"location":"computing/atomate/#configure-fireworks","text":"","title":"Configure Fireworks:"},{"location":"computing/atomate/#create-fireworks-configuration-files","text":"Make directory in environment\u2019s root directory (/envs/<name>/) called config cd ~/.conda/envs/cms mkdir config cd config Make 3 files: FW_config.yaml, db.json, and my_launchpad.yaml with the following contents. Replace the teal highlighted text with details specific to your configuration. Note that you can view your filesystem online .","title":"Create Fireworks configuration files"},{"location":"computing/atomate/#fireworks-configuration-fw_configyaml","text":"Nersc CONFIG_FILE_DIR: /global/homes/s/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Savio CONFIG_FILE_DIR: /global/home/users/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Lawrencium CONFIG_FILE_DIR: /global/home/users/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5","title":"Fireworks configuration (FW_config.yaml)"},{"location":"computing/atomate/#database-file-dbjson","text":"{ \"host\": \"mongodb03.NERSC.gov\", \"port\": 27017, \"database\": \"fw_db_name\", \"collection\": \"tasks\", \"admin_password\": \"<admin_password>\", \"admin_user\": \"<admin_username>\", \"readonly_password\": \"<readonly_password>\", \"readonly_user\": \"<readonly_username>\", \"aliases_config\": null }","title":"Database file (db.json)"},{"location":"computing/atomate/#fireworks-launchpad-file-my_launchpadyaml","text":"host: mongodb03.NERSC.gov port: 27017 name: 'fw_db_name' username: '<admin_username>' password: \u2018<admin_password>\u2019 logdir: null Istrm_lvl: DEBUG user_indices: [] wf_user_indices: []","title":"Fireworks LaunchPad file (my_launchpad.yaml)"},{"location":"computing/atomate/#create-fireworker-and-queue-launchers","text":"","title":"Create Fireworker and Queue Launchers:"},{"location":"computing/atomate/#fireworker-file-my_fworkeryaml","text":"Nersc name: NERSC_fworker category: '' query: '{}' env: db_file: /global/homes/s/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'srun -n 64 -c 4 --cpu_bind=cores vasp_std' gamma_vasp_cmd: 'srun -n 64 -c 4 --cpu_bind=cores vasp_gam' scratch_dir: /global/cscratch1/sd/sivonxay incar_update: Savio name: savio_fworker category: '' query: '{}' env: db_file: /global/home/users/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'mpirun --bind-to core vasp_std' gamma_vasp_cmd: 'mpirun --bind-to core vasp_gam' scratch_dir: /global/scratch/sivonxay/ incar_update: Lawrencium name: lrc_fworker category: '' query: '{}' env: db_file: /global/home/users/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'mpirun --bind-to core vasp_std' gamma_vasp_cmd: 'mpirun --bind-to core vasp_gam' scratch_dir: /global/scratch/sivonxay/ incar_update:","title":"Fireworker file (my_fworker.yaml)"},{"location":"computing/atomate/#queue-adapter-my_qadapteryaml","text":"Nersc _fw_name: CommonAdapter _fw_q_type: SLURM rocket_launch: rlaunch -w /global/homes/s/sivonxay/.conda/envs/cms/config/my_fworker.yaml singleshot nodes: 1 walltime: '24:00:00' account: matgen job_name: knl_launcher signal: SIGINT@60 qos: regular constraint: 'knl' pre_rocket: | source activate cms module load vasp-tpc/5.4.4-knl export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 Savio _fw_name: CommonAdapter _fw_q_type: SLURM rocket_launch: rlaunch -w /global/home/users/sivonxay/.conda/envs/cms/config/my_fworker.yaml singleshot nodes: 1 walltime: '24:00:00' account: co_lsdi job_name: knl_launcher queue: savio2_knl qos: lsdi_knl2_normal ntasks: 64 pre_rocket: | source activate cms module load vasp export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 post_rocket: null Lawrencium Available queues, partitions, and qos can be found at the following links: NERSC Savio Lawrencium Note: specifying singleshot in the queue adapter will limit each reserved job to running only one firework (even if other fireworks are ready and could run with your remaining wall time). Can change to rapidfire but this may result in lost runs (fireworks that do not complete because they run out of wall time). For more information on best practices for running VASP on multiple nodes (i.e. how to set vasp_cmd in my_fworker.yaml based on the number of nodes requested in my_qadapter) see the NERSC vasp training","title":"Queue Adapter (my_qadapter.yaml):"},{"location":"computing/atomate/#choose-the-appropriate-number-of-nodes-processes-and-coresprocess","text":"Follow this flowchart to decide how many nodes to request, the number of parallel processes to use, and the number of cores to use for each parallel process. The rule of thumb to remember is that you should use all of the cores for every node that you request. For example, if there are 256 cores per node, you request N nodes, n processes, and c cores/process, then the following relationship must hold: 256 N = n c Specify N in my_qadapter.yaml and specify n and c in my_fworker.yaml within your configuration folder.","title":"Choose the appropriate number of nodes, processes, and cores/process:"},{"location":"computing/atomate/#configure-bash-profile","text":"Append the following lines to the .bashrc.ext file (which is located in your home directory, e.g. /global/homes/s/sivonxay) Nersc module load python/3.6-anaconda-4.4 export PMG_VASP_PSP_DIR=/project/projectdirs/matgen/POTCARs export FW_CONFIG_FILE='/global/homes/s/sivonxay/.conda/envs/cms/config/FW_config.yaml' alias cdconfig='cd ~/.conda/envs/cms/config' Savio module load python/3.6 export PMG_VASP_PSP_DIR=/clusterfs/mp/software/POTCARs export FW_CONFIG_FILE='/global/home/users/sivonxay/.conda/envs/cms/config/FW_config.yaml' alias cdconfig='cd ~/.conda/envs/cms/config' Lawrencium Note the line alias cdconfig='cd ~/.conda/envs/cms/config' is optional but is recommended so you can more easily locate the directory with your configuration files stored by typing cdconfig into the command line.","title":"Configure Bash profile:"},{"location":"computing/atomate/#setup-api-key-and-add-potcar-directory-to-pymatgen","text":"Go to materialsproject.org and get an API key. Make sure you are using the environment setup above to run these commands. Nersc pmg config --add PMG_MAPI_KEY <USER_API_KEY> pmg config --add PMG_VASP_PSP_DIR /project/projectdirs/matgen/POTCARs Savio pmg config --add PMG_MAPI_KEY <USER_API_KEY> pmg config --add PMG_VASP_PSP_DIR /clusterfs/cloudcuckoo/POTCARs Running these commands will create a .pmgrc.yaml file (if it doesn\u2019t exist already) containing these configuration settings in your home directory","title":"Setup API key and Add POTCAR Directory to pymatgen"},{"location":"computing/atomate/#using-fireworks-and-atomate","text":"","title":"Using Fireworks and Atomate:"},{"location":"computing/atomate/#initializing-fireworks-database","text":"Use care when running this command. It will wipe all existing entries in your fireworks database in the fireworks, workflows, and launches collections. lpad reset","title":"Initializing Fireworks Database:"},{"location":"computing/atomate/#using-atomate-to-create-preset-workflow","text":"The following code blocks are python code that should be run in jupyter or from the terminal. Make sure you are using the environment setup above. The computer running the code should have access to mongodb03.NERSC.gov; this can be disregarded when running directly on NERSC or when connected to to the LBNL intranet. For computers outside of LBNL, a VPN will need to be used. Run the following python code by creating a file named \u201cmake_workflow.py\u201d: from pymatgen import MPRester from fireworks import LaunchPad from atomate.vasp.workflows.presets.core import wf_static from atomate.vasp import powerups mpr = MPRester() structure = mpr.get_structure_by_material_id(\u201cmp-145\u201d) wf = wf_static(structure) wf = powerups.add_common_powerups(wf, {\"scratch_dir\":\">>scratch_dir<<\"}) lp = LaunchPad.auto_load() lp.add_wf(wf) The previous python script added a static energy calculation for Hexagonal Lu to your job database. python \u201cmake_workflow.py To check on the job, run the command: lpad get_fws -s READY","title":"Using Atomate to create preset workflow"},{"location":"computing/atomate/#running-jobs","text":"We recommend using a tmux or screen when submitting jobs to preserve queue submission when the main ssh session is terminated. This allows one to keep the queue saturated with jobs. For example, for tmux: module load tmux tmux new -s background_launcher source active cms mkdir FireworksTest cd FireworksTest To exit the tmux session, press the \u201ccontrol\u201d and \u201cb\u201d keys together, followed by \u201cd\u201d To re-enter the tmux session: tmux attach -t background_launcher For more tmux commands, see the tmux cheatsheet Use qlaunch command to reserve jobs with SLURM\u2019s scheduler. Qlaunch has 3 modes; singleshot, rapidfire, and multi. Singleshot is used to launch one job, rapidfire is used to launch multiple jobs in quick succession, and multi creates one job with multiple fireworks runs. You\u2019ll probably want to use rapidfire. Some useful flags to set are: -m to specify maximum # of jobs in queue at any given time and --nlaunches to specify how many fireworks to run. Here are some examples: Example_1 qlaunch singleshot Example_2 qlaunch rapidfire --nlaunches 10 Example_3 qlaunch rapidfire --nlaunches infinite -m 50 Example_4 qlaunch -fm rapidfire --nlaunches infinite -m 50","title":"Running Jobs:"},{"location":"computing/atomate/#monitoring-database-fireworks","text":"Status of running fireworks can be determined by using the command Example_1 lpad get_fws -s RUNNING Example_2 lpad get_fws -q '{\"state\":\"RUNNING\", \"fw_id\":{\"$gte\": 50}}'","title":"Monitoring Database/ Fireworks"},{"location":"computing/atomate/#rerunning-fizzled-failed-fireworks","text":"Example_1 lpad rerun_fws -i <fw_id> Example_2 lpad rerun_fws -s FIZZLED Alternatively, you can view your jobs in the GUI by downloading your my_launchpad.yaml file to your local machine. To do so: scp <username>@dtn01.NERSC.gov:~/.conda/envs/cms/config/my_launchpad.yaml . Then, run this command, locally, in the folder containing the my_launchpad.yaml file. This will bring up the web interface hosted on you local machine on port 5000 (127.0.0.1:5000). lpad webgui Occasionally, jobs may hang and not complete. To check for hung jobs, use the command: lpad detect_lostruns To rerun these jobs, add the --rerun flag to the command. lpad detect_lostruns --rerun","title":"Rerunning fizzled (Failed) fireworks"},{"location":"computing/atomate/#viewing-database-and-outputs","text":"Download and install Robo 3T from the following link","title":"Viewing Database and Outputs:"},{"location":"computing/atomate/#setup-connection-using-mongodb-credentials","text":"","title":"Setup connection using mongodb credentials"},{"location":"computing/atomate/#querying-collections","text":"Authors: Eric Sivonxay, Julian Self, Ann Rutt, and Oxana Andriuc Contact: esivonxay@lbl.gov and jself@lbl.gov","title":"Querying collections"},{"location":"computing/hpc/","text":"Computing Resources Our group\u2019s main computing resources are: NERSC (the LBNL supercomputing center, one of the biggest in the world) Lawrencium / Berkeley Lab Research Computing Savio / Berkeley Research Computing Peregrine (the NREL supercomputing center) Argonne Leadership Computing Facility (sometimes) Oak Ridge Leadership Computing Facility (sometimes) At any time, if you feel you are computing-limited, please contact Kristin so she can work with you on finding solutions. NERSC To get started with calculations at NERSC: Ask Kristin about whether you will be running at NERSC and, if so, under what account / repository to charge. Request a NERSC account through the NERSC homepage (Google \u201cNERSC account request\u201d). A NERSC Liason or PI Proxy will validate your account and assign you an initial allocation of computing hours. At this point, you should be able to log in, check CPU-hour balances, etc. through \u201cNERSC NIM\u201d and \u201cMy NERSC\u201d portals In order to log in and run jobs on the various machines at NERSC, review the NERSC documentation. In order to load and submit scripts for various codes (VASP, ABINIT, Quantum Espresso), NERSC has lots of information to help. Try Google, e.g. \u201cNERSC VASP\u201d. ... * Note that for commercial codes such as VASP, there is an online form that allows you to enter your VASP license, which NERSC will confirm and then allow you access to. Please make a folder inside your project directory and submit all your jobs there, as your home folder has only about 40GB of space. For example, for matgen project, your work folder path should be something like the following: /global/project/projectdirs/matgen/YOUR_NERSC_USERNAME You can also request a mongo database for your project to be hosted on NERSC. Google \u201cMongoDB on NERSC\u201d for instructions. Donny Winston or Patrick Huck can also help you get set up and provide you with a preconfigured database suited for running Materials Project style workflows. Running Jobs on NERSC This tutorial provides a brief overview of setting yourself up to run jobs on NERSC. If any information is unclear or missing, feel free to edit this document or contact Kara Fong. Setting up a NERSC account: Contact the group\u2019s NERSC Liaison (currently Eric Sivonxay, see Group Jobs list). They will help you create an account and allocate you computational resources. You will then receive an email with instructions to fill out the Appropriate Use Policy form, set up your password, etc. Once your account is set up, you can manage it at the NERSC Information Management (NIM) website. Logging on (Setup): You must use the SSH protocol to connect to NERSC. Make sure you have SSH installed on your local computer (you can check this by typing which ssh ). Connecting with SSH: You must use the SSH protocol to connect to NERSC. Make sure you have SSH installed on your local computer (you can check this by typing which ssh) and that you have a directory named $HOME/.ssh on your local computer (if not, make it). You will also need to set up multi-factor authentication with NERSC . This will allow you to generate \"one time passwords\" (OTPs). You will need append a OTP to the end of your NIM password each time you log on to a NERSC cluster. We also advise you to configure a ssh socket so that you only have to log into NERSC with a OTP only once per session (helpful if you are scp-ing things). To do this: Create the directory ~/.ssh/sockets if it doesn't already exist. Open your ssh config file /.ssh/config (or create one if it doesn't exist) and add the following: Host *.nersc.gov ControlMaster auto ControlPath ~/.ssh/sockets/%r@%h-%p ControlPersist 600 You should now be ready to log on! You must store your SSH public key on the NERSC NIM database. Go to the NIM website, navigate to \u201cMy Stuff\u201d -> \u201cMy SSH Keys\u201d. Click on the SSH Keys tab. Copy your key (from id_rsa.pub) into the website\u2019s text box, click Add. Logging on: Log on to Cori, for example, by submitting the following command in the terminal: ssh username@cori.nersc.gov You will be prompted to enter your passphrase, which is your NIM password+OTP (e.g. <your_password><OTP> without any spaces). This will log you onto the cluster and take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias cori=\"ssh <your_username>@cori.nersc.gov\" Now you will be able to initialize a SSH connection to cori just by typing cori in the command line and pressing enter. Transferring files to/from NERSC: For small files, you can use SCP (secure copy). To get a file from NERSC, use: scp user_name@dtn01.nersc.gov:/remote/path/myfile.txt /local/path To send a file to NERSC, use: scp /local/path/myfile.txt user_name@dtn01.nersc.gov:/remote/path To move a larger quantity of data using a friendlier interface, use Globus Online . Running and monitoring jobs: The following instructions are for running on Cori. Analogous information for running on Edison can be found here. Most jobs are run in batch mode, in which you prepare a shell script telling the batch system how to run the job (number of nodes, time the job will run, etc.). NERSC\u2019s batch system software is called SLURM. Below is a simple batch script example, copied from the NERSC website: #!bin/bash -l #SBATCH -N 2 #Use 2 nodes #SBATCH -t 00:30:00 #Set 30 minute time limit #SBATCH -q regular #Submit to the regular QOS #SBATCH -L scratch #Job requires $SCRATCH file system #SBATCH -C haswell #Use Haswell nodes srun -n 32 -c 4 ./my_executable Here, the first line specifies which shell to use (in this case bash). The keyword #SBATCH is used to start directive lines ( click here for a full description of the sbatch options you can specify). The word \u201csrun\u201d starts execution of the code. To submit your batch script, use sbatch myscript.sl in the directory containing the script file. Below are some useful commands to control and monitor your jobs: sqs -u username (Lists jobs for your account) scancel job_id (Cancels a job from the queue) Choosing a QOS (quality of service): You specify which queue to use in your batch file. Use the debug queue for small, short test runs, the regular queue for production runs, and the premium queue for high-priority jobs. Choosing a node type (haswell vs knl): You may also specify the resource type you would like your job to run on, witnin your batch file. When running on Cori, there are two CPU architectures available, Haswell, and Knights Landing (known as KNL). Running on a Haswell node will afford you high individual core performance with up to 32 cores per node (or 64 threads per node). A KNL node provides a large core count (68 cores per node or 272 threads per node) which is suitable for programs capable of effectively utilizing multithreading. On Cori, there are 2388 Haswell nodes and 9688 KNL nodes. Automatic job submission on NERSC: crontab In order to automatically manage job submission at NERSC, you can use crontab. You can submit jobs periodically even when you are not signed in to any NERSC systems and perhaps reduce the queue time from 5-10 days to a few hours. This is possible because of the way jobs are managed in atomate/fireworks. Please make sure you feel comfortable submitting individual jobs via atomate before reading this section. In atomate, by using --maxloop 3 for example when setting rocket_launch in your my_qadapter.yaml, after 3 trials in each minute if there are still no READY jobs available in your Launchpad Fireworks would stop the running job on NERSC to avoid wasting computing resources. On the other hand, if you have Fireworks available with the READY state and you have been using crontab for a few days, even if the jobs you submitted a few days ago start running on NERSC, they would pull any READY Fireworks and start RUNNING them reducing the turnaround from a few days to a few hours! So how to setup crontab? Please follow the instructions here: 1. ssh to the node where you want to setup the crontab; try one that is easy to remember such as cori01 or edison01; for logging in to a specific node just do for example \u201cssh cori01\u201d after you log in to the system (Cori in this example). Type and enter: crontab -e Now you can setup the following command in the opened vi editor. What it does is basically running the SCRIPT.sh file every 120 minutes of every day of every week of every month of every year (or simply /120 * * *): */120 * * * * /bin/bash -l PATH_TO_SCRIPT.sh >> PATH_TO_LOGFILE Setup your SCRIPT.sh like the following: (as a suggestion, you can simply put this file and the log file which keeps a log of submission states in your home folder): source activate YOUR_PRODUCTION_CONDA_ENVIRONMENT FW_CONFIG_FILE=PATH_TO_CONFIG_DIR/FW_config.yaml cd PATH_TO_YOUR_PRODUCTION_FOLDER qlaunch --fill_mode rapidfire -m 1000 --nlaunches 1 The last line of this 3-line file is really what submitting your job inside your production folder with the settings that you set in FW_config.yaml file. See atomate documentation for more info. Please make sure to set your PRODUCTION_FOLDER under /global/project/projectdirs/ that has much more space than your home folder and it is also backed up. Make sure to keep an eye on how close you are to disk space and file number limitations by checking https://my.nersc.gov/ periodically. Running Jupyter Notebooks on Cori Jupyter notebooks are quickly becoming an indispensable tool for doing computational science. In some cases, you might want to (or need to) harness NERSC computing power inside of a jupyter notebook. To do this, you can use NERSC\u2019s new Jupyterhub system at https://jupyter-dev.nersc.gov/. These notebooks are run on a large memory node of Cori and can also submit jobs to the batch queues (see http://bit.ly/2A0mqrl for details). All of your files and the project directory will be accessible from the Jupyterhub, but your conda envs won\u2019t be available before you do some configuration. To set up a conda environment so it is accessible from the Jupyterhub, activate the environment and setup an ipython kernel. To do this, run the command \u201cpip install ipykernel\u201d. More info can be found at http://bit.ly/2yoKAzB. Automatic Job Packing with FireWorks DISCLAIMER: Only use job packing if you have trouble with typical job submission. The following tip is not 100% guaranteed to work., and is based on limited, subjective experience on Cori. Talk to Alex Dunn (ardunn@lbl.gov) for help if you have trouble. The Cori queue system can be unreasonably slow when submitting many (e.g., hundreds, thousands) of small (e.g., single node or 2 nodes) jobs with qos-normal priority on Haswell. In practice, we have found that the Cori job scheduler will give your jobs low throughput if you have many jobs in queue, and you will often only be able to run 5-30 jobs at a time, while the rest wait in queue for far longer than originally expected (e.g., weeks). While there is no easy way to increase your queue submission rate (AFAIK), you can use FireWorks job-packing to \u201ctrick\u201d Cori\u2019s SLURM scheduler into running many jobs in serial on many parallel compute nodes with a single queue submission, vastly increasing throughput. You can use job packing with the \u201cmulti\u201d option to rlaunch. This command launches N parallel python processes on the Cori scheduling node, each which runs a job using M compute nodes. The steps to job packing are: 1. Edit your my_qadapter.yaml file to reserve N * M nodes for each submission. For example, if each of your jobs takes M = 2 nodes, and you want a N = 10 x speedup, reserve 20 nodes per queue submission. 2. Change your rlaunch command to: rlaunch -c /your/config multi N To have each FireWorks process run as many jobs as possible in serial before the walltime, use the --nlaunches 0 option. To prevent FireWorks from submitting jobs with little walltime left (causing jobs to frequently get stuck as \u201cRUNNING\u201d), set the --timeout option. Make sure --timeout is set so that even a long running job submitted at the end of your allocation will not run over your walltime limit. Your my_qadapter.yaml should then have something similar to the following lines: rocket_launch: rlaunch -c /your/config multi 10 --nlaunches 0 --timeout 169200 nodes: 20 Typically, setting N <= 10 will give you a good N-times speedup with no problems. There are no guarantees, however, when N > 10-20. Use N > 50 at your own risk! Berkeley Research Computing Berkeley Research Computing (BRC) hosts the Savio supercomputing cluster. Savio operates on a condo computing model, in which many PI's and researchers purchase nodes to add to the system. Nodes are accessible to all who have access to the system, though priority access will be given to contributors of the specific nodes. BRC provides 3 types of allocations: Condo - Priority access for nodes contributed by the condo group. Faculty Computing Allowance (FCA) - Limited computing time provided to each Faculty member using Savio. Setting up a BRC account Please make sure you will actually be performing work on Savio before requesting an account. To get an account on Savio, fill out the form linked below, making sure to select the appropriate allocation. Typically, most students and postdocs will be running on co_lsdi. For more information, visit (Berkeley Research Computing)[http://research-it.berkeley.edu/services/high-performance-computing] After your account is made, you'll need to set up 2-factor authentication . This will allow you to generate \"one time passwords\" (OTPs). You will need append a OTP to the end of your NIM password each time you log on to a NERSC cluster. We recommend using Google Authenticator, although any OTP manager will work. Logging on (Setup): You must use the SSH protocol to connect to BRC. Make sure you have SSH installed on your local computer (you can check this by typing which ssh ). Make sure you have a directory named $HOME/.ssh on your local computer (if not, make it). We also advise you to configure a ssh socket so that you only have to log into BRC with a OTP only once per session (helpful if you are scp-ing things). To do this: Create the directory ~/.ssh/sockets if it doesn't already exist. Open your ssh config file /.ssh/config (or create one if it doesn't exist) and add the following: Host *.brc.berekeley.edu ControlMaster auto ControlPath ~/.ssh/sockets/%r@%h-%p ControlPersist 600 After your account is made, you'll need to set up 2-factor authentication. We recommend using Google Authenticator, although any OTP manager will work. You should now be ready to log on! Logging on to BRC To access your shiny new savio account, you'll want to SSH onto the system from a terminal. ssh your_username@hpc.brc.berekeley.edu You will be prompted to enter your passphrase+OTP (e.g. <your_password><OTP> without any spaces). This will take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias savio=\"ssh your_username@hpc.brc.berekeley.edu\" Now you will be able to initialize a SSH connection to Savio just by typing savio in the command line and pressing enter. Running on BRC Under the condo account co_lsdi, we have exclusive access to 28 KNL nodes. Additionally, we have the ability to run on other nodes at low priority mode. Accessing Software binaries Software within BRC is managed through modules. You can access precompiled, preinstalled software by loading the desired module. module load <module_name> To view a list of currently installed programs, use the following command: module avail To view the currently loaded modules use the command: module list Software modules can be removed by using either of the following two commands: module unload <module_name> module purge Accessing In-House software packages The Persson Group maintains their own versions of popular codes such as VASP, GAUSSIAN, QCHEM and LAMMPS. To access these binaries, ensure that you have the proper licenses and permissions, then append the following line to the .bashrc file in your root directory: export MODULEPATH=${MODULEPATH}:/global/home/groups/co_lsdi/sl7/modfiles Using Persson Group Owned KNL nodes To run on the KNL nodes, use the following job script, replacing with the desired job executable name. To run vasp after loading the proper module, use vasp_std, vasp_gam, or vasp_ncl. #!bin/bash -l #SBATCH --nodes=1 #Use 1 node #SBATCH --ntasks=64 #Use 64 tasks for the job #SBATCH --qos=lsdi_knl2_normal #Set job to normal qos #SBATCH --time=01:00:00 #Set 1 hour time limit for job #SBATCH --partition=savio2_knl #Submit to the KNL nodes owned by the Persson Group #SBATCH --account=co_lsdi #Charge to co_lsdi accout #SBATCH --job-name=savio2_job #Name for the job mpirun --bind-to core <executable> Running on Haswell Nodes(on Low Priority) To run on Haswell nodes, use the following slurm submission script: #!bin/bash -l #SBATCH --nodes=1 #Use 1 node #SBATCH --ntasks_per_core=1 #Use 1 task per core on the node #SBATCH --qos=savio_lowprio #Set job to low priority qos #SBATCH --time=01:00:00 #Set 1 hour time limit for job #SBATCH --partition=savio2 #Submit to the haswell nodes #SBATCH --account=co_lsdi #Charge to co_lsdi accout #SBATCH --job-name=savio2_job #Name for the job mpirun --bind-to core <executable> LBNL Laboratory Research Computing Lawrence Berkeley National Laboratory's Laboratory Research Computing (LRC) hosts the Lawrencium supercomputing cluster. LRC operates on a condo computing model, in which many PI's and researchers purchase nodes to add to the system. Nodes are accessible to all who have access to the system, though priority access will be given to contributors of the specific nodes. BRC provides 3 types of allocations: Condo - Priority access for nodes contributed by the condo group. PI Computing Allowance (PCA) - Limited computing time provided to each PI member using Lawrencium. Persson Group ES1 GPU Node Specs: GPU Computing Node Processors Dual-socket, 4-core, 2.6GHz Intel 4112 processors (8 cores/node) Memory 192GB (8 X 8GB) 2400Mhz DDR4 RDIMMs Interconnect 56Gb/s Mellanox ConnectX5 EDR Infiniband interconnect GPU 2 ea. Nvidia Tesla v100 accelerator boards Hard Drive 500GB SSD (Local swap and log files) Setting up a LRC account Please make sure you will actually be performing work on Lawrencium before requesting an account. To get an account on Lawrencium, fill out this form and the user agreement to a one-time password token generator and your account. You will also need to set up a MFA token for your account . Before logging on (setup) You must use the SSH protocol to connect to Lawrencium. Make sure you have SSH installed on your local computer (you can check this by typing which ssh ). Make sure you have a directory named $HOME/.ssh on your local computer (if not, make it). After your account is made, you'll need to set up 2-factor authentication. We recommend using Google Authenticator, although any OTP manager will work. You should now be ready to log on! Logging on to LRC To access your shiny new Lawrencium account, you'll want to SSH onto the system from a terminal. ssh your_username@lrc-login.lbl.gov You will be prompted to enter your pin+OTP (e.g. <your_pin><OTP> without any spaces). This will take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias lawrencium=\"ssh <your_username>@lrc-login.lbl.gov\" Now you will be able to initialize a SSH connection to Savio just by typing lawrencium in the command line and pressing enter. Running on LRC Under the condo accounts condo_mp_cf1 (56 cf1 nodes) and condo_mp_es1 (1 gpu node), we have exclusive access to certain Lawrencium nodes. If you do not know which of these node groups you are supposed to be running on, you probably shouldn't be running on Lawrencium. Additionally, we have the ability to run on ES1 GPU nodes at low priority mode (es_lowprio). Accessing Software binaries Software within LRC is managed through modules. You can access precompiled, preinstalled software by loading the desired module. module load <module_name> To view a list of currently installed programs, use the following command: module avail To view the currently loaded modules use the command: module list Software modules can be removed by using either of the following two commands: module unload <module_name> module purge Using Persson Group Owned LRC nodes To run on the nodes, use the following job script, replacing with the desired job executable name. Cf1_Knl #!/bin/bash # Job name: #SBATCH --job-name=<job_name> # # Partition: #SBATCH --partition=cf1 # # QoS: #SBATCH --qos=condo_mp_cf1 # # Account: #SBATCH --account=lr_mp # # Nodes (IF YOU CHANGE THIS YOU MUST CHANGE ntasks too!!!): #SBATCH --nodes=1 # # Processors (MUST BE 64xNUM_NODES ALWAYS!!!): #SBATCH --ntasks=64 # # Wall clock limit: #SBATCH --time=24:00:00 ## Run command module load vasp/6.prerelease-vdw export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 # NEVER CHANGE THIS!! mpirun --bind-to core <executable> Es1_Condo #!/bin/bash # Job name: #SBATCH --job-name=<job_name> # # Partition: #SBATCH --partition=es1 # # QoS: #SBATCH --qos=condo_mp_es1 # # Account: #SBATCH --account=lr_mp # # GPUs: #SBATCH --gres=gpu:2 # # CPU cores: #SBATCH --cpus-per-task=8 # # Constraints: #SBATCH --constraint=es1_v100 # # Wall clock limit: #SBATCH --time=24:00:00 export CUDA_VISIBLE_DEVICES=0,1 module load cuda/10.0 Es1_Lowprio #!/bin/bash # Job name: #SBATCH --job-name=<job_name> # # Partition: #SBATCH --partition=es1 # # QoS: #SBATCH --qos=es_lowprio # # Account: #SBATCH --account=lr_mp # # GPUs: #SBATCH --gres=gpu:2 # # CPU cores: #SBATCH --cpus-per-task=8 # # Constraints: #SBATCH --constraint=es1_v100 # # Wall clock limit: #SBATCH --time=24:00:00 export CUDA_VISIBLE_DEVICES=0,1 module load cuda/10.0 Interactive Jobs on the Group GPU Condo Node To run an interactive session on the GPU node, use the following two commands to provision and log in to the node: salloc --time=24:00:00 --nodes=1 -p es1 --gres=gpu:2 --cpus-per-task=8 --qos=condo_mp_es1 --account=lr_mp srun --pty -u bash -i Additional resources Other Persson group members and the NERSC website are both excellent resources for getting additional help. If that fails, you can reach out to the NERSC Operations staff: 1-800-666-3772 (or 1-510-486-8600) Computer Operations = menu option 1 (24/7) Account Support = menu option 2, accounts@nersc.gov HPC Consulting = menu option 3, or consult@nersc.gov Authors: Kara Fong, Eric Sivonxay, John Dagdelen Contact: karafong@lbl.gov","title":"High Performance Computing"},{"location":"computing/hpc/#computing-resources","text":"Our group\u2019s main computing resources are: NERSC (the LBNL supercomputing center, one of the biggest in the world) Lawrencium / Berkeley Lab Research Computing Savio / Berkeley Research Computing Peregrine (the NREL supercomputing center) Argonne Leadership Computing Facility (sometimes) Oak Ridge Leadership Computing Facility (sometimes) At any time, if you feel you are computing-limited, please contact Kristin so she can work with you on finding solutions.","title":"Computing Resources "},{"location":"computing/hpc/#nersc","text":"","title":"NERSC "},{"location":"computing/hpc/#to-get-started-with-calculations-at-nersc","text":"Ask Kristin about whether you will be running at NERSC and, if so, under what account / repository to charge. Request a NERSC account through the NERSC homepage (Google \u201cNERSC account request\u201d). A NERSC Liason or PI Proxy will validate your account and assign you an initial allocation of computing hours. At this point, you should be able to log in, check CPU-hour balances, etc. through \u201cNERSC NIM\u201d and \u201cMy NERSC\u201d portals In order to log in and run jobs on the various machines at NERSC, review the NERSC documentation. In order to load and submit scripts for various codes (VASP, ABINIT, Quantum Espresso), NERSC has lots of information to help. Try Google, e.g. \u201cNERSC VASP\u201d. ... * Note that for commercial codes such as VASP, there is an online form that allows you to enter your VASP license, which NERSC will confirm and then allow you access to. Please make a folder inside your project directory and submit all your jobs there, as your home folder has only about 40GB of space. For example, for matgen project, your work folder path should be something like the following: /global/project/projectdirs/matgen/YOUR_NERSC_USERNAME You can also request a mongo database for your project to be hosted on NERSC. Google \u201cMongoDB on NERSC\u201d for instructions. Donny Winston or Patrick Huck can also help you get set up and provide you with a preconfigured database suited for running Materials Project style workflows.","title":"To get started with calculations at NERSC: "},{"location":"computing/hpc/#running-jobs-on-nersc","text":"This tutorial provides a brief overview of setting yourself up to run jobs on NERSC. If any information is unclear or missing, feel free to edit this document or contact Kara Fong.","title":"Running Jobs on NERSC"},{"location":"computing/hpc/#setting-up-a-nersc-account","text":"Contact the group\u2019s NERSC Liaison (currently Eric Sivonxay, see Group Jobs list). They will help you create an account and allocate you computational resources. You will then receive an email with instructions to fill out the Appropriate Use Policy form, set up your password, etc. Once your account is set up, you can manage it at the NERSC Information Management (NIM) website.","title":"Setting up a NERSC account:"},{"location":"computing/hpc/#logging-on-setup","text":"You must use the SSH protocol to connect to NERSC. Make sure you have SSH installed on your local computer (you can check this by typing which ssh ).","title":"Logging on (Setup):"},{"location":"computing/hpc/#connecting-with-ssh","text":"You must use the SSH protocol to connect to NERSC. Make sure you have SSH installed on your local computer (you can check this by typing which ssh) and that you have a directory named $HOME/.ssh on your local computer (if not, make it). You will also need to set up multi-factor authentication with NERSC . This will allow you to generate \"one time passwords\" (OTPs). You will need append a OTP to the end of your NIM password each time you log on to a NERSC cluster. We also advise you to configure a ssh socket so that you only have to log into NERSC with a OTP only once per session (helpful if you are scp-ing things). To do this: Create the directory ~/.ssh/sockets if it doesn't already exist. Open your ssh config file /.ssh/config (or create one if it doesn't exist) and add the following: Host *.nersc.gov ControlMaster auto ControlPath ~/.ssh/sockets/%r@%h-%p ControlPersist 600 You should now be ready to log on! You must store your SSH public key on the NERSC NIM database. Go to the NIM website, navigate to \u201cMy Stuff\u201d -> \u201cMy SSH Keys\u201d. Click on the SSH Keys tab. Copy your key (from id_rsa.pub) into the website\u2019s text box, click Add.","title":"Connecting with SSH:"},{"location":"computing/hpc/#logging-on","text":"Log on to Cori, for example, by submitting the following command in the terminal: ssh username@cori.nersc.gov You will be prompted to enter your passphrase, which is your NIM password+OTP (e.g. <your_password><OTP> without any spaces). This will log you onto the cluster and take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias cori=\"ssh <your_username>@cori.nersc.gov\" Now you will be able to initialize a SSH connection to cori just by typing cori in the command line and pressing enter.","title":"Logging on:"},{"location":"computing/hpc/#transferring-files-tofrom-nersc","text":"For small files, you can use SCP (secure copy). To get a file from NERSC, use: scp user_name@dtn01.nersc.gov:/remote/path/myfile.txt /local/path To send a file to NERSC, use: scp /local/path/myfile.txt user_name@dtn01.nersc.gov:/remote/path To move a larger quantity of data using a friendlier interface, use Globus Online .","title":"Transferring files to/from NERSC:"},{"location":"computing/hpc/#running-and-monitoring-jobs","text":"The following instructions are for running on Cori. Analogous information for running on Edison can be found here. Most jobs are run in batch mode, in which you prepare a shell script telling the batch system how to run the job (number of nodes, time the job will run, etc.). NERSC\u2019s batch system software is called SLURM. Below is a simple batch script example, copied from the NERSC website: #!bin/bash -l #SBATCH -N 2 #Use 2 nodes #SBATCH -t 00:30:00 #Set 30 minute time limit #SBATCH -q regular #Submit to the regular QOS #SBATCH -L scratch #Job requires $SCRATCH file system #SBATCH -C haswell #Use Haswell nodes srun -n 32 -c 4 ./my_executable Here, the first line specifies which shell to use (in this case bash). The keyword #SBATCH is used to start directive lines ( click here for a full description of the sbatch options you can specify). The word \u201csrun\u201d starts execution of the code. To submit your batch script, use sbatch myscript.sl in the directory containing the script file. Below are some useful commands to control and monitor your jobs: sqs -u username (Lists jobs for your account) scancel job_id (Cancels a job from the queue)","title":"Running and monitoring jobs:"},{"location":"computing/hpc/#choosing-a-qos-quality-of-service","text":"You specify which queue to use in your batch file. Use the debug queue for small, short test runs, the regular queue for production runs, and the premium queue for high-priority jobs.","title":"Choosing a QOS (quality of service):"},{"location":"computing/hpc/#choosing-a-node-type-haswell-vs-knl","text":"You may also specify the resource type you would like your job to run on, witnin your batch file. When running on Cori, there are two CPU architectures available, Haswell, and Knights Landing (known as KNL). Running on a Haswell node will afford you high individual core performance with up to 32 cores per node (or 64 threads per node). A KNL node provides a large core count (68 cores per node or 272 threads per node) which is suitable for programs capable of effectively utilizing multithreading. On Cori, there are 2388 Haswell nodes and 9688 KNL nodes.","title":"Choosing a node type (haswell vs knl):"},{"location":"computing/hpc/#automatic-job-submission-on-nersc-crontab","text":"In order to automatically manage job submission at NERSC, you can use crontab. You can submit jobs periodically even when you are not signed in to any NERSC systems and perhaps reduce the queue time from 5-10 days to a few hours. This is possible because of the way jobs are managed in atomate/fireworks. Please make sure you feel comfortable submitting individual jobs via atomate before reading this section. In atomate, by using --maxloop 3 for example when setting rocket_launch in your my_qadapter.yaml, after 3 trials in each minute if there are still no READY jobs available in your Launchpad Fireworks would stop the running job on NERSC to avoid wasting computing resources. On the other hand, if you have Fireworks available with the READY state and you have been using crontab for a few days, even if the jobs you submitted a few days ago start running on NERSC, they would pull any READY Fireworks and start RUNNING them reducing the turnaround from a few days to a few hours! So how to setup crontab? Please follow the instructions here: 1. ssh to the node where you want to setup the crontab; try one that is easy to remember such as cori01 or edison01; for logging in to a specific node just do for example \u201cssh cori01\u201d after you log in to the system (Cori in this example). Type and enter: crontab -e Now you can setup the following command in the opened vi editor. What it does is basically running the SCRIPT.sh file every 120 minutes of every day of every week of every month of every year (or simply /120 * * *): */120 * * * * /bin/bash -l PATH_TO_SCRIPT.sh >> PATH_TO_LOGFILE Setup your SCRIPT.sh like the following: (as a suggestion, you can simply put this file and the log file which keeps a log of submission states in your home folder): source activate YOUR_PRODUCTION_CONDA_ENVIRONMENT FW_CONFIG_FILE=PATH_TO_CONFIG_DIR/FW_config.yaml cd PATH_TO_YOUR_PRODUCTION_FOLDER qlaunch --fill_mode rapidfire -m 1000 --nlaunches 1 The last line of this 3-line file is really what submitting your job inside your production folder with the settings that you set in FW_config.yaml file. See atomate documentation for more info. Please make sure to set your PRODUCTION_FOLDER under /global/project/projectdirs/ that has much more space than your home folder and it is also backed up. Make sure to keep an eye on how close you are to disk space and file number limitations by checking https://my.nersc.gov/ periodically.","title":"Automatic job submission on NERSC: crontab "},{"location":"computing/hpc/#running-jupyter-notebooks-on-cori","text":"Jupyter notebooks are quickly becoming an indispensable tool for doing computational science. In some cases, you might want to (or need to) harness NERSC computing power inside of a jupyter notebook. To do this, you can use NERSC\u2019s new Jupyterhub system at https://jupyter-dev.nersc.gov/. These notebooks are run on a large memory node of Cori and can also submit jobs to the batch queues (see http://bit.ly/2A0mqrl for details). All of your files and the project directory will be accessible from the Jupyterhub, but your conda envs won\u2019t be available before you do some configuration. To set up a conda environment so it is accessible from the Jupyterhub, activate the environment and setup an ipython kernel. To do this, run the command \u201cpip install ipykernel\u201d. More info can be found at http://bit.ly/2yoKAzB.","title":"Running Jupyter Notebooks on Cori "},{"location":"computing/hpc/#automatic-job-packing-with-fireworks","text":"DISCLAIMER: Only use job packing if you have trouble with typical job submission. The following tip is not 100% guaranteed to work., and is based on limited, subjective experience on Cori. Talk to Alex Dunn (ardunn@lbl.gov) for help if you have trouble. The Cori queue system can be unreasonably slow when submitting many (e.g., hundreds, thousands) of small (e.g., single node or 2 nodes) jobs with qos-normal priority on Haswell. In practice, we have found that the Cori job scheduler will give your jobs low throughput if you have many jobs in queue, and you will often only be able to run 5-30 jobs at a time, while the rest wait in queue for far longer than originally expected (e.g., weeks). While there is no easy way to increase your queue submission rate (AFAIK), you can use FireWorks job-packing to \u201ctrick\u201d Cori\u2019s SLURM scheduler into running many jobs in serial on many parallel compute nodes with a single queue submission, vastly increasing throughput. You can use job packing with the \u201cmulti\u201d option to rlaunch. This command launches N parallel python processes on the Cori scheduling node, each which runs a job using M compute nodes. The steps to job packing are: 1. Edit your my_qadapter.yaml file to reserve N * M nodes for each submission. For example, if each of your jobs takes M = 2 nodes, and you want a N = 10 x speedup, reserve 20 nodes per queue submission. 2. Change your rlaunch command to: rlaunch -c /your/config multi N To have each FireWorks process run as many jobs as possible in serial before the walltime, use the --nlaunches 0 option. To prevent FireWorks from submitting jobs with little walltime left (causing jobs to frequently get stuck as \u201cRUNNING\u201d), set the --timeout option. Make sure --timeout is set so that even a long running job submitted at the end of your allocation will not run over your walltime limit. Your my_qadapter.yaml should then have something similar to the following lines: rocket_launch: rlaunch -c /your/config multi 10 --nlaunches 0 --timeout 169200 nodes: 20 Typically, setting N <= 10 will give you a good N-times speedup with no problems. There are no guarantees, however, when N > 10-20. Use N > 50 at your own risk!","title":"Automatic Job Packing with FireWorks "},{"location":"computing/hpc/#berkeley-research-computing","text":"Berkeley Research Computing (BRC) hosts the Savio supercomputing cluster. Savio operates on a condo computing model, in which many PI's and researchers purchase nodes to add to the system. Nodes are accessible to all who have access to the system, though priority access will be given to contributors of the specific nodes. BRC provides 3 types of allocations: Condo - Priority access for nodes contributed by the condo group. Faculty Computing Allowance (FCA) - Limited computing time provided to each Faculty member using Savio.","title":"Berkeley Research Computing "},{"location":"computing/hpc/#setting-up-a-brc-account","text":"Please make sure you will actually be performing work on Savio before requesting an account. To get an account on Savio, fill out the form linked below, making sure to select the appropriate allocation. Typically, most students and postdocs will be running on co_lsdi. For more information, visit (Berkeley Research Computing)[http://research-it.berkeley.edu/services/high-performance-computing] After your account is made, you'll need to set up 2-factor authentication . This will allow you to generate \"one time passwords\" (OTPs). You will need append a OTP to the end of your NIM password each time you log on to a NERSC cluster. We recommend using Google Authenticator, although any OTP manager will work.","title":"Setting up a BRC account"},{"location":"computing/hpc/#logging-on-setup_1","text":"You must use the SSH protocol to connect to BRC. Make sure you have SSH installed on your local computer (you can check this by typing which ssh ). Make sure you have a directory named $HOME/.ssh on your local computer (if not, make it). We also advise you to configure a ssh socket so that you only have to log into BRC with a OTP only once per session (helpful if you are scp-ing things). To do this: Create the directory ~/.ssh/sockets if it doesn't already exist. Open your ssh config file /.ssh/config (or create one if it doesn't exist) and add the following: Host *.brc.berekeley.edu ControlMaster auto ControlPath ~/.ssh/sockets/%r@%h-%p ControlPersist 600 After your account is made, you'll need to set up 2-factor authentication. We recommend using Google Authenticator, although any OTP manager will work. You should now be ready to log on!","title":"Logging on (Setup):"},{"location":"computing/hpc/#logging-on-to-brc","text":"To access your shiny new savio account, you'll want to SSH onto the system from a terminal. ssh your_username@hpc.brc.berekeley.edu You will be prompted to enter your passphrase+OTP (e.g. <your_password><OTP> without any spaces). This will take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias savio=\"ssh your_username@hpc.brc.berekeley.edu\" Now you will be able to initialize a SSH connection to Savio just by typing savio in the command line and pressing enter.","title":"Logging on to BRC"},{"location":"computing/hpc/#running-on-brc","text":"Under the condo account co_lsdi, we have exclusive access to 28 KNL nodes. Additionally, we have the ability to run on other nodes at low priority mode.","title":"Running on BRC"},{"location":"computing/hpc/#accessing-software-binaries","text":"Software within BRC is managed through modules. You can access precompiled, preinstalled software by loading the desired module. module load <module_name> To view a list of currently installed programs, use the following command: module avail To view the currently loaded modules use the command: module list Software modules can be removed by using either of the following two commands: module unload <module_name> module purge","title":"Accessing Software binaries"},{"location":"computing/hpc/#accessing-in-house-software-packages","text":"The Persson Group maintains their own versions of popular codes such as VASP, GAUSSIAN, QCHEM and LAMMPS. To access these binaries, ensure that you have the proper licenses and permissions, then append the following line to the .bashrc file in your root directory: export MODULEPATH=${MODULEPATH}:/global/home/groups/co_lsdi/sl7/modfiles","title":"Accessing In-House software packages"},{"location":"computing/hpc/#using-persson-group-owned-knl-nodes","text":"To run on the KNL nodes, use the following job script, replacing with the desired job executable name. To run vasp after loading the proper module, use vasp_std, vasp_gam, or vasp_ncl. #!bin/bash -l #SBATCH --nodes=1 #Use 1 node #SBATCH --ntasks=64 #Use 64 tasks for the job #SBATCH --qos=lsdi_knl2_normal #Set job to normal qos #SBATCH --time=01:00:00 #Set 1 hour time limit for job #SBATCH --partition=savio2_knl #Submit to the KNL nodes owned by the Persson Group #SBATCH --account=co_lsdi #Charge to co_lsdi accout #SBATCH --job-name=savio2_job #Name for the job mpirun --bind-to core <executable>","title":"Using Persson Group Owned KNL nodes"},{"location":"computing/hpc/#running-on-haswell-nodeson-low-priority","text":"To run on Haswell nodes, use the following slurm submission script: #!bin/bash -l #SBATCH --nodes=1 #Use 1 node #SBATCH --ntasks_per_core=1 #Use 1 task per core on the node #SBATCH --qos=savio_lowprio #Set job to low priority qos #SBATCH --time=01:00:00 #Set 1 hour time limit for job #SBATCH --partition=savio2 #Submit to the haswell nodes #SBATCH --account=co_lsdi #Charge to co_lsdi accout #SBATCH --job-name=savio2_job #Name for the job mpirun --bind-to core <executable>","title":"Running on Haswell Nodes(on Low Priority)"},{"location":"computing/hpc/#lbnl-laboratory-research-computing","text":"Lawrence Berkeley National Laboratory's Laboratory Research Computing (LRC) hosts the Lawrencium supercomputing cluster. LRC operates on a condo computing model, in which many PI's and researchers purchase nodes to add to the system. Nodes are accessible to all who have access to the system, though priority access will be given to contributors of the specific nodes. BRC provides 3 types of allocations: Condo - Priority access for nodes contributed by the condo group. PI Computing Allowance (PCA) - Limited computing time provided to each PI member using Lawrencium.","title":"LBNL Laboratory Research Computing "},{"location":"computing/hpc/#persson-group-es1-gpu-node-specs","text":"GPU Computing Node Processors Dual-socket, 4-core, 2.6GHz Intel 4112 processors (8 cores/node) Memory 192GB (8 X 8GB) 2400Mhz DDR4 RDIMMs Interconnect 56Gb/s Mellanox ConnectX5 EDR Infiniband interconnect GPU 2 ea. Nvidia Tesla v100 accelerator boards Hard Drive 500GB SSD (Local swap and log files)","title":"Persson Group ES1 GPU Node Specs:"},{"location":"computing/hpc/#setting-up-a-lrc-account","text":"Please make sure you will actually be performing work on Lawrencium before requesting an account. To get an account on Lawrencium, fill out this form and the user agreement to a one-time password token generator and your account. You will also need to set up a MFA token for your account .","title":"Setting up a LRC account"},{"location":"computing/hpc/#before-logging-on-setup","text":"You must use the SSH protocol to connect to Lawrencium. Make sure you have SSH installed on your local computer (you can check this by typing which ssh ). Make sure you have a directory named $HOME/.ssh on your local computer (if not, make it). After your account is made, you'll need to set up 2-factor authentication. We recommend using Google Authenticator, although any OTP manager will work. You should now be ready to log on!","title":"Before logging on (setup)"},{"location":"computing/hpc/#logging-on-to-lrc","text":"To access your shiny new Lawrencium account, you'll want to SSH onto the system from a terminal. ssh your_username@lrc-login.lbl.gov You will be prompted to enter your pin+OTP (e.g. <your_pin><OTP> without any spaces). This will take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias lawrencium=\"ssh <your_username>@lrc-login.lbl.gov\" Now you will be able to initialize a SSH connection to Savio just by typing lawrencium in the command line and pressing enter.","title":"Logging on to LRC"},{"location":"computing/hpc/#running-on-lrc","text":"Under the condo accounts condo_mp_cf1 (56 cf1 nodes) and condo_mp_es1 (1 gpu node), we have exclusive access to certain Lawrencium nodes. If you do not know which of these node groups you are supposed to be running on, you probably shouldn't be running on Lawrencium. Additionally, we have the ability to run on ES1 GPU nodes at low priority mode (es_lowprio).","title":"Running on LRC"},{"location":"computing/hpc/#accessing-software-binaries_1","text":"Software within LRC is managed through modules. You can access precompiled, preinstalled software by loading the desired module. module load <module_name> To view a list of currently installed programs, use the following command: module avail To view the currently loaded modules use the command: module list Software modules can be removed by using either of the following two commands: module unload <module_name> module purge","title":"Accessing Software binaries"},{"location":"computing/hpc/#using-persson-group-owned-lrc-nodes","text":"To run on the nodes, use the following job script, replacing with the desired job executable name. Cf1_Knl #!/bin/bash # Job name: #SBATCH --job-name=<job_name> # # Partition: #SBATCH --partition=cf1 # # QoS: #SBATCH --qos=condo_mp_cf1 # # Account: #SBATCH --account=lr_mp # # Nodes (IF YOU CHANGE THIS YOU MUST CHANGE ntasks too!!!): #SBATCH --nodes=1 # # Processors (MUST BE 64xNUM_NODES ALWAYS!!!): #SBATCH --ntasks=64 # # Wall clock limit: #SBATCH --time=24:00:00 ## Run command module load vasp/6.prerelease-vdw export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 # NEVER CHANGE THIS!! mpirun --bind-to core <executable> Es1_Condo #!/bin/bash # Job name: #SBATCH --job-name=<job_name> # # Partition: #SBATCH --partition=es1 # # QoS: #SBATCH --qos=condo_mp_es1 # # Account: #SBATCH --account=lr_mp # # GPUs: #SBATCH --gres=gpu:2 # # CPU cores: #SBATCH --cpus-per-task=8 # # Constraints: #SBATCH --constraint=es1_v100 # # Wall clock limit: #SBATCH --time=24:00:00 export CUDA_VISIBLE_DEVICES=0,1 module load cuda/10.0 Es1_Lowprio #!/bin/bash # Job name: #SBATCH --job-name=<job_name> # # Partition: #SBATCH --partition=es1 # # QoS: #SBATCH --qos=es_lowprio # # Account: #SBATCH --account=lr_mp # # GPUs: #SBATCH --gres=gpu:2 # # CPU cores: #SBATCH --cpus-per-task=8 # # Constraints: #SBATCH --constraint=es1_v100 # # Wall clock limit: #SBATCH --time=24:00:00 export CUDA_VISIBLE_DEVICES=0,1 module load cuda/10.0","title":"Using Persson Group Owned LRC nodes"},{"location":"computing/hpc/#interactive-jobs-on-the-group-gpu-condo-node","text":"To run an interactive session on the GPU node, use the following two commands to provision and log in to the node: salloc --time=24:00:00 --nodes=1 -p es1 --gres=gpu:2 --cpus-per-task=8 --qos=condo_mp_es1 --account=lr_mp srun --pty -u bash -i","title":"Interactive Jobs on the Group GPU Condo Node"},{"location":"computing/hpc/#additional-resources","text":"Other Persson group members and the NERSC website are both excellent resources for getting additional help. If that fails, you can reach out to the NERSC Operations staff: 1-800-666-3772 (or 1-510-486-8600) Computer Operations = menu option 1 (24/7) Account Support = menu option 2, accounts@nersc.gov HPC Consulting = menu option 3, or consult@nersc.gov Authors: Kara Fong, Eric Sivonxay, John Dagdelen Contact: karafong@lbl.gov","title":"Additional resources"},{"location":"computing/md/","text":"LAMMPS More Information to come GROMACS More Information to come","title":"Molecular Dynamics"},{"location":"computing/md/#lammps","text":"More Information to come","title":"LAMMPS"},{"location":"computing/md/#gromacs","text":"More Information to come","title":"GROMACS"},{"location":"computing/new_computer/","text":"Setting up a new Macbook Upgrade your OS If your computer is not using the latest OS, you should upgrade to the latest OS first. Installing Python development environment The best way to manage Python installations these days is a \u201cconda env\u201d. This will allow you to manage different Python \u201cenvironments\u201d, where each environment is a set of libraries that you have installed. For example, you can have one environment that uses Python 2.7 and has certain library versions installed, and another environment that uses Python 3.5 and has other libraries installed. Another advantage of conda environments is that you can apply the same procedure on NERSC and other computing centers that support conda. To do this, follow the online instructions on installing a conda environment and see modifications below: * http://conda.pydata.org/docs/using/index.html * (probably) prefer to install the \u201cminiconda\u201d version rather than anaconda * (probably) prefer to install \u201cminiconda 3\u201d rather than \u201cminiconda 2\u201d. Both will work fine and allow you to do everything the other one does so don\u2019t stress too much about this decision. * When creating environments, use a command like this (note that this also installs recommended libraries): conda create --name py3 python=3 numpy matplotlib seaborn plotly pandas flask pymongo scipy sympy scikit-learn jupyter If you want a reference guide to conda commands, try: http://conda.pydata.org/docs/using/cheatsheet.html Install high-throughput computation environment Our group has a set of base codebases used for performing high-throughput calculations. Note that if your project does not involve high-throughput calculation, you may need only one or two of these libraries installed \u2013 ask your subgroup head if you are unsure. After activating a conda environment , install the following packages using a combination of git clone >>REPO_NAME<< and python setup.py develop. Start with: git clone https://www.github.com/materialsproject/fireworks You might need to generate an ssh key for the git clone command to work: ssh-keygen -t rsa -b 4096 no password is probably OK unless you are security conscious add your SSH key to your Github profile Then: cd fireworks; python setup.py develop; cd .. Repeat the process above for the remaining libraries: git clone https://www.github.com/materialsproject/pymatgen cd pymatgen; python setup.py develop; cd .. git clone https://www.github.com/materialsproject/pymatgen-db cd pymatgen-db; python setup.py develop; cd .. git clone https://www.github.com/materialsproject/custodian cd custodian; python setup.py develop; cd .. Repeat the same process for a couple of other libraries on the hackingmaterials github site: git clone https://www.github.com/hackingmaterials/atomate cd atomate; python setup.py develop; cd .. git clone https://www.github.com/hackingmaterials/matminer cd matminer; python setup.py develop; cd .. If you want, you can automatically source activate your environment in your .bash_profile file. This will automatically load your environment when you open a Terminal. Otherwise, you will start off in your default Mac Python. Other things to do Bring the now-empty box to 143C, write your name on it, and add it to the tower. Set up your Time Machine backup (make sure you have purchased or received an external hard disk). https://support.apple.com/en-us/HT204412 You can also set up an online backup plan (e.g., Crashplan or Backblaze) to provide you with a second backup. Install MongoDB. Install Docker. Purchase Microsoft office from LBNL software distribution.","title":"Computer Setup"},{"location":"computing/new_computer/#setting-up-a-new-macbook","text":"","title":"Setting up a new Macbook "},{"location":"computing/new_computer/#upgrade-your-os","text":"If your computer is not using the latest OS, you should upgrade to the latest OS first.","title":"Upgrade your OS"},{"location":"computing/new_computer/#installing-python-development-environment","text":"The best way to manage Python installations these days is a \u201cconda env\u201d. This will allow you to manage different Python \u201cenvironments\u201d, where each environment is a set of libraries that you have installed. For example, you can have one environment that uses Python 2.7 and has certain library versions installed, and another environment that uses Python 3.5 and has other libraries installed. Another advantage of conda environments is that you can apply the same procedure on NERSC and other computing centers that support conda. To do this, follow the online instructions on installing a conda environment and see modifications below: * http://conda.pydata.org/docs/using/index.html * (probably) prefer to install the \u201cminiconda\u201d version rather than anaconda * (probably) prefer to install \u201cminiconda 3\u201d rather than \u201cminiconda 2\u201d. Both will work fine and allow you to do everything the other one does so don\u2019t stress too much about this decision. * When creating environments, use a command like this (note that this also installs recommended libraries): conda create --name py3 python=3 numpy matplotlib seaborn plotly pandas flask pymongo scipy sympy scikit-learn jupyter If you want a reference guide to conda commands, try: http://conda.pydata.org/docs/using/cheatsheet.html","title":"Installing Python development environment"},{"location":"computing/new_computer/#install-high-throughput-computation-environment","text":"Our group has a set of base codebases used for performing high-throughput calculations. Note that if your project does not involve high-throughput calculation, you may need only one or two of these libraries installed \u2013 ask your subgroup head if you are unsure. After activating a conda environment , install the following packages using a combination of git clone >>REPO_NAME<< and python setup.py develop. Start with: git clone https://www.github.com/materialsproject/fireworks You might need to generate an ssh key for the git clone command to work: ssh-keygen -t rsa -b 4096 no password is probably OK unless you are security conscious add your SSH key to your Github profile Then: cd fireworks; python setup.py develop; cd .. Repeat the process above for the remaining libraries: git clone https://www.github.com/materialsproject/pymatgen cd pymatgen; python setup.py develop; cd .. git clone https://www.github.com/materialsproject/pymatgen-db cd pymatgen-db; python setup.py develop; cd .. git clone https://www.github.com/materialsproject/custodian cd custodian; python setup.py develop; cd .. Repeat the same process for a couple of other libraries on the hackingmaterials github site: git clone https://www.github.com/hackingmaterials/atomate cd atomate; python setup.py develop; cd .. git clone https://www.github.com/hackingmaterials/matminer cd matminer; python setup.py develop; cd .. If you want, you can automatically source activate your environment in your .bash_profile file. This will automatically load your environment when you open a Terminal. Otherwise, you will start off in your default Mac Python.","title":"Install high-throughput computation environment"},{"location":"computing/new_computer/#other-things-to-do","text":"Bring the now-empty box to 143C, write your name on it, and add it to the tower. Set up your Time Machine backup (make sure you have purchased or received an external hard disk). https://support.apple.com/en-us/HT204412 You can also set up an online backup plan (e.g., Crashplan or Backblaze) to provide you with a second backup. Install MongoDB. Install Docker. Purchase Microsoft office from LBNL software distribution.","title":"Other things to do"},{"location":"computing/software/","text":"Our software stack A brief summary of our software stack includes: pymatgen / pymatgen-db - for representing and analyzing crystal structures, as well as setting up/performing manual calculations FireWorks - for executing and managing calculation workflows at supercomputing centers custodian - instead of directly running an executable like VASP, one can wrap the executable in custodian to detect and fix errors atomate - for quickly defining multiple types of materials science workflows matminer - for large data analysis and visualization We also heavily use the Materials Project database. Additional software used by many members of our group include: VESTA - Ovito - VMD - Studio 3T - PyCharm - iTerm2 - To learn how to use the software stack, you can consult the documentation of the individual codebases as well as review the following resources: The 2018 Materials Project workshop (note that MatMethods is now called atomate): https://github.com/materialsproject/workshop-2018 The 2014 Materials Virtual Lab presentations: https://materialsvirtuallab.org/software/ The Materials Project YouTube tutorials: https://www.youtube.com/user/MaterialsProject If you have a specific question, sometimes the easiest solution is to post it to the Slack group and crowdsource the answer (or just ask Shyam). Software help groups If you have problems with software, and in particular the software maintained by our group and our collaborators, you should contact the appropriate help group. The documentation for the software will list what that channel is; if not, try the Github Issues page. If you are reaching out for help, try to provide everything needed to quickly reproduce and debug the problem (files, test code, etc). Two other ways to get software help that are more self-guided are: * If you are having trouble using a particular class or function, look for unit tests within the code, which often demonstrate how to use the class or function * If the class or function has a unique name (e.g., MaterialsProjectCompatibility), another option is to both Google and search on github.com for the particular class/function. The github.com search will often reveal code snippets from users all around the world. Authors: John Dagdelen, Eric Sivonxay Contact: jdagdelen@lbl.gov","title":"Software"},{"location":"computing/software/#our-software-stack","text":"A brief summary of our software stack includes: pymatgen / pymatgen-db - for representing and analyzing crystal structures, as well as setting up/performing manual calculations FireWorks - for executing and managing calculation workflows at supercomputing centers custodian - instead of directly running an executable like VASP, one can wrap the executable in custodian to detect and fix errors atomate - for quickly defining multiple types of materials science workflows matminer - for large data analysis and visualization We also heavily use the Materials Project database. Additional software used by many members of our group include: VESTA - Ovito - VMD - Studio 3T - PyCharm - iTerm2 - To learn how to use the software stack, you can consult the documentation of the individual codebases as well as review the following resources: The 2018 Materials Project workshop (note that MatMethods is now called atomate): https://github.com/materialsproject/workshop-2018 The 2014 Materials Virtual Lab presentations: https://materialsvirtuallab.org/software/ The Materials Project YouTube tutorials: https://www.youtube.com/user/MaterialsProject If you have a specific question, sometimes the easiest solution is to post it to the Slack group and crowdsource the answer (or just ask Shyam).","title":"Our software stack"},{"location":"computing/software/#software-help-groups","text":"If you have problems with software, and in particular the software maintained by our group and our collaborators, you should contact the appropriate help group. The documentation for the software will list what that channel is; if not, try the Github Issues page. If you are reaching out for help, try to provide everything needed to quickly reproduce and debug the problem (files, test code, etc). Two other ways to get software help that are more self-guided are: * If you are having trouble using a particular class or function, look for unit tests within the code, which often demonstrate how to use the class or function * If the class or function has a unique name (e.g., MaterialsProjectCompatibility), another option is to both Google and search on github.com for the particular class/function. The github.com search will often reveal code snippets from users all around the world. Authors: John Dagdelen, Eric Sivonxay Contact: jdagdelen@lbl.gov","title":"Software help groups "},{"location":"getting_started/after/","text":"After You Arrive Welcome to Berkeley and the Persson Group! Here is a checklist to help you get started once you have recieved your access to LBL either as an employee or affiliate. Useful Quick Links Persson Group Website: People Internal Group Site lbl.gov A-Z Index UC Berkeley ERSO Intranet UC Berkeley UCPath Payroll, benefits, and HR system First Day Checklist Very Basics & High Priority Administrative Tasks The Onboarding Team will communicate with you by email to coordinate having a group member meet you the first time you arrive in Building 33. They will help you find your desk and provide a short walk through to become familiar with the building and introduce you to other group members. Once you make your way to Building 33, ask around to find the group member you are looking for if you are having trouble. Follow the instructions received during your orientation or by email to activate your Berkeley Lab Identity Account (aka LDAP). This log-in will be used for most LBL online resources. Connect to the internet using the lbnl-visitor WiFi network (it is open access). Try out your newly created LDAP log-in. Connect to the lbnl-employee WiFi network , open your @lbl.gov email, google calendar, etc. Send out an email introducing yourself to perssongroup@lbl.gov (this is the email list for everyone in the Persson group) from your @lbl.gov email address. This is a tradition to help welcome new group members since it can take some time to meet everyone in person. This also serves as a notice for the group to help initiate various onboarding tasks. Here is the intro email questions template and an example introduction email (note permissions to view these documents are tied to your @lbl.gov google account). See Alice Muller with your Employee ID to introduce yourself and set-up B33 after hours access (or send an email including the ID number on your LBL badge). By default, you will NOT have off-hours site access to Building 33 (i.e. from 5:30pm to 7am on weekdays, on weekends, holidays) unless a request is submitted. Prioritize completing the GERT (General Radiological Worker Training) before the end of your first day. Get your project/funding specific information for making purchases: Purchasing with LBL Funds: Ask Sonia Dominguez for the project id, activity id, and SAS Approver associated with your funding to use Ebuy. Purchasing with UC Berkeley Funds: To make purchases using UC Berkeley funds, you will need the chart string associated with that funding source which can be obtained from UC Berkeley Research Administration Staff for the College of Engineering. Email Miriam Kader including that your PI is Kristin Persson and ask for the chart string associated with your funding source. Begin setting up your desk (ideally before completing the ergonomic self-assessment). Order a computer as soon as possible if you have not done so already. See Getting a Computer and Workspace Set-up for guidelines on selecting a computer, needed accessories, and other equipment. When you are ready, check-out Purchasing for more information on placing your order. Ask Rebecca Stern (or others on the Onboarding Team) about checking out the the group's inventory of unclaimed items. First Week Checklist Also start working through the other checklists and information below when possible Send an email to Ryan Kingsbury requesting to be added to the Persson Group email lists and team google drive (if you do not have access already). Use the subject line \"Onboarding: [YOUR NAME]\". Complete all LBL/UC Berkeley training courses including the ergonomic self-assessment. Note the GERT must be completed on your first day. Also note that UC Berkeley required training courses will not be assigned until after your UCPath profile is set up, which can take 1-2 weeks after your start date. Orient yourself with LBL's calendar system (Google Calendar). Update your LBL calendar with your schedule so other group members can view your availability. Check to ensure your calendar is populated with needed meetings (group meetings, subgroup meetings, etc.) Contact a meeting organizer to invite you to needed calendar events. Join the Berkeley Theory slack channel using your @lbl.gov email. Slack is a messaging app that is used across multiple groups and researchers at LBL to communicate about computational materials research, The Materials Project, and other collaborative efforts. Send an email to Koki Muraoka with a headshot and tiny bio for the group website (bit of humor in the tiny bio is very welcome). Explore the group\u2019s Team Google Drive. Note some of these documents populate the group's internal website . Add your name and information to the Persson Group roster Print out an updated name tag for your desk. If needed, work with Alice Muller to update the name tag for your office. Research Introduction Checklist Meet with Kristin to plan an appropriate introduction for your specific research, project management, subgroup, etc. Ask to read the proposal that funds your work. This will help explain the impact of your project, the long-term plans and goals, and how your project fits in with other efforts. Get familiar with your subgroup and any other individuals you will be working closely with for your research. Generally take the time to learn about other group member's research focuses and areas of expertise to get a sense of who may be your best resource for different questions and who you may be supporting if you are a more senior group member. Computer & IT Checklist Install the lab VPN for connecting to the lab network from home. For example, this lets you download research articles from home. See https://software.lbl.gov for instructions on installation. Contact Eric Sivonxay about getting user accounts set-up for any computing resources you may be using (NERSC, Savio, VASP, etc.). Get connected to the printer. The printer is located in 143C and instructions for how to connect can be found on the internal group site. Julian Self is the current printer czar and can advise you if you are having any difficulties. Begin setting up your research computer. See setting up you computer and our internal group site for some installation tutorials and recommended softwares. Payroll Basics LBL Employees LBL employees (staff, postdocs, and graduate students) must enter their time on a regular basis using the LETS payroll system (total hours for a single month are reported). You will need the project ID and activity ID associated with the funding source for your income/stipend. Sonia Dominguez sends time entering instructions before each payroll deadline and can address any related questions. LBL Payroll Resources UC Berkeley Employees / LBL Affiliates UC Berkeley employees (staff, postdocs, and graduate students) must enter their time monthly using the CalTime payroll system. Note: Sometimes your initial leave balances (24 PTO days and 12 Sick Days) do not populate correctly in CalTime until after the 2nd paycheck processes. If this is an issue (i.e., if you need to use leave during your first pay period), contact ersopayroll@erso.berkeley.edu. CalTime Information on UCPath Payroll and Benefits System Graduate Students: UC Berkeley's ERSO (Engineering Resrouce Support Organization) handles processing GSR appointment requests. These appointments are used to issue stipends (no need to report hours in a payroll system) so keep an eye out for email reminders each Fall, Spring, and Summer.","title":"After you Arrive"},{"location":"getting_started/after/#after-you-arrive","text":"Welcome to Berkeley and the Persson Group! Here is a checklist to help you get started once you have recieved your access to LBL either as an employee or affiliate.","title":"After You Arrive"},{"location":"getting_started/after/#useful-quick-links","text":"Persson Group Website: People Internal Group Site lbl.gov A-Z Index UC Berkeley ERSO Intranet UC Berkeley UCPath Payroll, benefits, and HR system","title":"Useful Quick Links"},{"location":"getting_started/after/#first-day-checklist","text":"Very Basics & High Priority Administrative Tasks The Onboarding Team will communicate with you by email to coordinate having a group member meet you the first time you arrive in Building 33. They will help you find your desk and provide a short walk through to become familiar with the building and introduce you to other group members. Once you make your way to Building 33, ask around to find the group member you are looking for if you are having trouble. Follow the instructions received during your orientation or by email to activate your Berkeley Lab Identity Account (aka LDAP). This log-in will be used for most LBL online resources. Connect to the internet using the lbnl-visitor WiFi network (it is open access). Try out your newly created LDAP log-in. Connect to the lbnl-employee WiFi network , open your @lbl.gov email, google calendar, etc. Send out an email introducing yourself to perssongroup@lbl.gov (this is the email list for everyone in the Persson group) from your @lbl.gov email address. This is a tradition to help welcome new group members since it can take some time to meet everyone in person. This also serves as a notice for the group to help initiate various onboarding tasks. Here is the intro email questions template and an example introduction email (note permissions to view these documents are tied to your @lbl.gov google account). See Alice Muller with your Employee ID to introduce yourself and set-up B33 after hours access (or send an email including the ID number on your LBL badge). By default, you will NOT have off-hours site access to Building 33 (i.e. from 5:30pm to 7am on weekdays, on weekends, holidays) unless a request is submitted. Prioritize completing the GERT (General Radiological Worker Training) before the end of your first day. Get your project/funding specific information for making purchases: Purchasing with LBL Funds: Ask Sonia Dominguez for the project id, activity id, and SAS Approver associated with your funding to use Ebuy. Purchasing with UC Berkeley Funds: To make purchases using UC Berkeley funds, you will need the chart string associated with that funding source which can be obtained from UC Berkeley Research Administration Staff for the College of Engineering. Email Miriam Kader including that your PI is Kristin Persson and ask for the chart string associated with your funding source. Begin setting up your desk (ideally before completing the ergonomic self-assessment). Order a computer as soon as possible if you have not done so already. See Getting a Computer and Workspace Set-up for guidelines on selecting a computer, needed accessories, and other equipment. When you are ready, check-out Purchasing for more information on placing your order. Ask Rebecca Stern (or others on the Onboarding Team) about checking out the the group's inventory of unclaimed items.","title":"First Day Checklist"},{"location":"getting_started/after/#first-week-checklist","text":"Also start working through the other checklists and information below when possible Send an email to Ryan Kingsbury requesting to be added to the Persson Group email lists and team google drive (if you do not have access already). Use the subject line \"Onboarding: [YOUR NAME]\". Complete all LBL/UC Berkeley training courses including the ergonomic self-assessment. Note the GERT must be completed on your first day. Also note that UC Berkeley required training courses will not be assigned until after your UCPath profile is set up, which can take 1-2 weeks after your start date. Orient yourself with LBL's calendar system (Google Calendar). Update your LBL calendar with your schedule so other group members can view your availability. Check to ensure your calendar is populated with needed meetings (group meetings, subgroup meetings, etc.) Contact a meeting organizer to invite you to needed calendar events. Join the Berkeley Theory slack channel using your @lbl.gov email. Slack is a messaging app that is used across multiple groups and researchers at LBL to communicate about computational materials research, The Materials Project, and other collaborative efforts. Send an email to Koki Muraoka with a headshot and tiny bio for the group website (bit of humor in the tiny bio is very welcome). Explore the group\u2019s Team Google Drive. Note some of these documents populate the group's internal website . Add your name and information to the Persson Group roster Print out an updated name tag for your desk. If needed, work with Alice Muller to update the name tag for your office.","title":"First Week Checklist"},{"location":"getting_started/after/#research-introduction-checklist","text":"Meet with Kristin to plan an appropriate introduction for your specific research, project management, subgroup, etc. Ask to read the proposal that funds your work. This will help explain the impact of your project, the long-term plans and goals, and how your project fits in with other efforts. Get familiar with your subgroup and any other individuals you will be working closely with for your research. Generally take the time to learn about other group member's research focuses and areas of expertise to get a sense of who may be your best resource for different questions and who you may be supporting if you are a more senior group member.","title":"Research Introduction Checklist"},{"location":"getting_started/after/#computer-it-checklist","text":"Install the lab VPN for connecting to the lab network from home. For example, this lets you download research articles from home. See https://software.lbl.gov for instructions on installation. Contact Eric Sivonxay about getting user accounts set-up for any computing resources you may be using (NERSC, Savio, VASP, etc.). Get connected to the printer. The printer is located in 143C and instructions for how to connect can be found on the internal group site. Julian Self is the current printer czar and can advise you if you are having any difficulties. Begin setting up your research computer. See setting up you computer and our internal group site for some installation tutorials and recommended softwares.","title":"Computer &amp; IT Checklist"},{"location":"getting_started/after/#payroll-basics","text":"","title":"Payroll Basics"},{"location":"getting_started/after/#lbl-employees","text":"LBL employees (staff, postdocs, and graduate students) must enter their time on a regular basis using the LETS payroll system (total hours for a single month are reported). You will need the project ID and activity ID associated with the funding source for your income/stipend. Sonia Dominguez sends time entering instructions before each payroll deadline and can address any related questions. LBL Payroll Resources","title":"LBL Employees"},{"location":"getting_started/after/#uc-berkeley-employees-lbl-affiliates","text":"UC Berkeley employees (staff, postdocs, and graduate students) must enter their time monthly using the CalTime payroll system. Note: Sometimes your initial leave balances (24 PTO days and 12 Sick Days) do not populate correctly in CalTime until after the 2nd paycheck processes. If this is an issue (i.e., if you need to use leave during your first pay period), contact ersopayroll@erso.berkeley.edu. CalTime Information on UCPath Payroll and Benefits System Graduate Students: UC Berkeley's ERSO (Engineering Resrouce Support Organization) handles processing GSR appointment requests. These appointments are used to issue stipends (no need to report hours in a payroll system) so keep an eye out for email reminders each Fall, Spring, and Summer.","title":"UC Berkeley Employees / LBL Affiliates"},{"location":"getting_started/before/","text":"Before You Arrive A member of the Persson Group Onboarding Team will reach out before your expected start date to share this group handbook and coordinate other onboarding efforts to welcome you to the Persson Group. Although many things can only be taken care of after arriving, here are a few simple things you can do in advance. If you have not been contacted by the Persson Group Onboarding Team at least one week before your arrival, please reach out to Ryan Kingsbury and Ann Rutt (contact information is on the Persson Group website in Useful Quick Links below). We do our best, but there may have been a miscommunication or oversight on our end if no one has not reached out two weeks before your expected start date. Work Scope and Hiring Details Several administrative and research details in onboarding are uniquie to each group member. Some of this information may have been communicated during your hiring process, but please reach out to Kristin (or members of the Persson Group Onboarding Team) if you are unsure about any of the following questions before your arrival: What project(s) will I be working on? What funding source will support my work? Will I be hired as a UC Berkeley employee (and LBL affiliate) or LBL employee? Which subgroup(s) will I be joining? Who will I be working most closely with? Many administrative matters (HR orientation, payroll, purchasing, etc.) will vary depending on if you are a UC Berkeley or LBL employee. This information which help you navigate joining the Persson Group such as understanding which administrative systems you will use and who can help you become orientated with your research/work. Human Resources Orientation As mentioned above, the HR processes and orientation will vary depending on whether your financial support will come from UC Berkeley or LBL controlled funding sources. Please refer to the relevant information below depending on your situation. LBL Employees There are a few steps that must be completed before your LBL new hire orientation can be scheduled (completing a formal application, background check, etc.). Contact Alice Muller with any questions or status update requests for matters on the Persson Group end during this time. Once everything has been successfully processed, you should recieve an email from LBL HR regarding an in person new hire appointment with a general national lab orientation and to complete new hire paperwork. This appointment is held at the national lab and further instructions will be sent regarding location, accessing the lab, and getting your badge afterwards. UC Berkeley Employees Once your hiring paperwork with UC Berkeley has been processed, you should recieve an email regarding an in person new hire appointment with UC Berkeley HR to complete new hire paperwork. After this appointment, it may take 1-2 weeks for your information to be populated into UC Berkeley's HR and payroll system ( UCPath ). See Obtaining Your Cal 1 Card at https://cal1card.berkeley.edu/ for information on how to get your Cal1 card (which serves as an ID card for UC Berkeley). Postdocs will need to wait until after your new hire paperwork meeting and after you have an active profile in UCPath to go to the Cal1 Card Office. For information on setting-up your CalNet ID click here . Your CalNet ID is used for authentification for UC Berkeley resources. Postdocs will need to wait until after their UC Berkeley new hire paperwork and UCPath information has been processed before they can activate their CalNet ID. Proceed to follow the instructions for LBL Affiliates Due to Kristin's joint appointment, all graduate students and postdocs hired through UC Berkeley also have LBL Affiliate status to access the group's offices and resources at the national lab. LBL Affiliates On the Persson Group end, a LBL Affiliate appointment must be processed by Alice Muller before you can obtain your LBL badge. Once your appointment has been processed, you will recieve email instructions from LBL for getting your LBL badge and setting up your LDAP account (used for authentification for national lab resources). Unlike LBL employees, no in person orientation with LBL is required. Note that your LBL Affiliate appointment does not depend on having a Calnet ID or Cal 1 Card; this can be processed immediately after you complete your UC Berkeley hiring paperwork. If your start date is coming up soon, but you have not recieved any email instructions from LBL, contact Alice Muller asking for an update on the status of your affiliate appointment. Order a Computer Most long-term appointments (graduate student, postdoc, staff) involve purchasing a new computer. It typically takes at least 2 weeks for a computer to arrive. To avoid delays once you start from waiting to recieve your computer, you can make arrangements to order one in advance. Work with Alice Muller to handle the purchase (your access to purchasing resources is granted after your arrival). Visit the Getting a Computer section of the handbook for group guidelines on computer selection and purchasing. Short-term appointments (undergraduate students, internships, etc.) will not involve a computer purchase unless otherwise stated. Instead, an excellent computer can be borrowed from the group\u2019s inventory. Onboarding Points of Contact Postdoc & Staff Onboarding : Ryan Kingsbury, 33-142G Graduate Student Onboarding : Ann Rutt, 33-143G LBL Administrative Assistance : Alice Mueller, 33-122B (amuller2@lbl.gov) LBL Funding & Payroll Assistance : Sonia Dominguez, 33-122B (scdominguez@lbl.gov) UC Berkeley Administrative Assistance : David Anderssen, Krystle Bartholomew, or Vincent Ianniello, ERSO HR Operations, 199ME Cory Hall (ersohrops@erso.berkeley.edu) UC Berkeley Funding & Payroll Assistance : Miriam Kader, 353 Cory Hall (mkader@berkeley.edu) Desk Assignments : Rebecca Stern, 33-143F Persson Group Website : See People's page for Group Member emails","title":"Before you Arrive"},{"location":"getting_started/before/#before-you-arrive","text":"A member of the Persson Group Onboarding Team will reach out before your expected start date to share this group handbook and coordinate other onboarding efforts to welcome you to the Persson Group. Although many things can only be taken care of after arriving, here are a few simple things you can do in advance. If you have not been contacted by the Persson Group Onboarding Team at least one week before your arrival, please reach out to Ryan Kingsbury and Ann Rutt (contact information is on the Persson Group website in Useful Quick Links below). We do our best, but there may have been a miscommunication or oversight on our end if no one has not reached out two weeks before your expected start date.","title":"Before You Arrive"},{"location":"getting_started/before/#work-scope-and-hiring-details","text":"Several administrative and research details in onboarding are uniquie to each group member. Some of this information may have been communicated during your hiring process, but please reach out to Kristin (or members of the Persson Group Onboarding Team) if you are unsure about any of the following questions before your arrival: What project(s) will I be working on? What funding source will support my work? Will I be hired as a UC Berkeley employee (and LBL affiliate) or LBL employee? Which subgroup(s) will I be joining? Who will I be working most closely with? Many administrative matters (HR orientation, payroll, purchasing, etc.) will vary depending on if you are a UC Berkeley or LBL employee. This information which help you navigate joining the Persson Group such as understanding which administrative systems you will use and who can help you become orientated with your research/work.","title":"Work Scope and Hiring Details"},{"location":"getting_started/before/#human-resources-orientation","text":"As mentioned above, the HR processes and orientation will vary depending on whether your financial support will come from UC Berkeley or LBL controlled funding sources. Please refer to the relevant information below depending on your situation.","title":"Human Resources Orientation"},{"location":"getting_started/before/#lbl-employees","text":"There are a few steps that must be completed before your LBL new hire orientation can be scheduled (completing a formal application, background check, etc.). Contact Alice Muller with any questions or status update requests for matters on the Persson Group end during this time. Once everything has been successfully processed, you should recieve an email from LBL HR regarding an in person new hire appointment with a general national lab orientation and to complete new hire paperwork. This appointment is held at the national lab and further instructions will be sent regarding location, accessing the lab, and getting your badge afterwards.","title":"LBL Employees"},{"location":"getting_started/before/#uc-berkeley-employees","text":"Once your hiring paperwork with UC Berkeley has been processed, you should recieve an email regarding an in person new hire appointment with UC Berkeley HR to complete new hire paperwork. After this appointment, it may take 1-2 weeks for your information to be populated into UC Berkeley's HR and payroll system ( UCPath ). See Obtaining Your Cal 1 Card at https://cal1card.berkeley.edu/ for information on how to get your Cal1 card (which serves as an ID card for UC Berkeley). Postdocs will need to wait until after your new hire paperwork meeting and after you have an active profile in UCPath to go to the Cal1 Card Office. For information on setting-up your CalNet ID click here . Your CalNet ID is used for authentification for UC Berkeley resources. Postdocs will need to wait until after their UC Berkeley new hire paperwork and UCPath information has been processed before they can activate their CalNet ID. Proceed to follow the instructions for LBL Affiliates Due to Kristin's joint appointment, all graduate students and postdocs hired through UC Berkeley also have LBL Affiliate status to access the group's offices and resources at the national lab.","title":"UC Berkeley Employees"},{"location":"getting_started/before/#lbl-affiliates","text":"On the Persson Group end, a LBL Affiliate appointment must be processed by Alice Muller before you can obtain your LBL badge. Once your appointment has been processed, you will recieve email instructions from LBL for getting your LBL badge and setting up your LDAP account (used for authentification for national lab resources). Unlike LBL employees, no in person orientation with LBL is required. Note that your LBL Affiliate appointment does not depend on having a Calnet ID or Cal 1 Card; this can be processed immediately after you complete your UC Berkeley hiring paperwork. If your start date is coming up soon, but you have not recieved any email instructions from LBL, contact Alice Muller asking for an update on the status of your affiliate appointment.","title":"LBL Affiliates"},{"location":"getting_started/before/#order-a-computer","text":"Most long-term appointments (graduate student, postdoc, staff) involve purchasing a new computer. It typically takes at least 2 weeks for a computer to arrive. To avoid delays once you start from waiting to recieve your computer, you can make arrangements to order one in advance. Work with Alice Muller to handle the purchase (your access to purchasing resources is granted after your arrival). Visit the Getting a Computer section of the handbook for group guidelines on computer selection and purchasing. Short-term appointments (undergraduate students, internships, etc.) will not involve a computer purchase unless otherwise stated. Instead, an excellent computer can be borrowed from the group\u2019s inventory.","title":"Order a Computer"},{"location":"getting_started/before/#onboarding-points-of-contact","text":"Postdoc & Staff Onboarding : Ryan Kingsbury, 33-142G Graduate Student Onboarding : Ann Rutt, 33-143G LBL Administrative Assistance : Alice Mueller, 33-122B (amuller2@lbl.gov) LBL Funding & Payroll Assistance : Sonia Dominguez, 33-122B (scdominguez@lbl.gov) UC Berkeley Administrative Assistance : David Anderssen, Krystle Bartholomew, or Vincent Ianniello, ERSO HR Operations, 199ME Cory Hall (ersohrops@erso.berkeley.edu) UC Berkeley Funding & Payroll Assistance : Miriam Kader, 353 Cory Hall (mkader@berkeley.edu) Desk Assignments : Rebecca Stern, 33-143F Persson Group Website : See People's page for Group Member emails","title":"Onboarding Points of Contact"},{"location":"getting_started/buy_computer/","text":"Getting a Computer Purchasing a new computer is budgeted for when hiring long-term appointments (graduate student, postdoc, staff). Short-term appointments (undergraduate students, internships, etc.) will not involve a computer purchase unless otherwise stated as a computer can be borrowed from the group\u2019s inventory instead. It typically takes at least 2 weeks for a computer to arrive so it is recommended to prioritize placing an order either before your arrival or as soon as possible after you start. Why Purchase a Macbook Pro? Buying a high end Macbook Pro is highly advised although not mandatory. In our experience, Macs offer the best systems for our work and having the same operating system within our group makes it easier to troubleshoot and share code. Using computers with similar hardware can also make it easier to share and pass down computer accessories and equipment. While we want to be conscious of cost and avoid wastefulness, the value of using well performing equipment that better supports our productivity, collaborative group culture, and reduces wasted time adds value which greatly outweighs the initial financial cost. If you would like to get anything other than a Macbook Pro, please consult more senior group members first. You can use the Apple website to browse details and see what is available in through the supplier, Anacapa, in Ebuy and Bearbuy. Many group members use a 13\u201d or 15\" Macbook Pro. The 13\" is powerful enough to do serious work and is lighter and smaller than the 15\" for improved mobility. However, the most powerful computing hardware is typically only available in the 15\" if you want to prioritize performance. Making the Purchase If you are ordering a computer before your arrival, you will need to coordinate with Alice Mueller (amuller2@lbl.gov) to handle the purchase. Please send an email with the details of your selections. Otherwise, if you are ordering a computer after you have started, you can make arrangements yourself. See purchasing for more information.","title":"Getting a Computer"},{"location":"getting_started/buy_computer/#getting-a-computer","text":"Purchasing a new computer is budgeted for when hiring long-term appointments (graduate student, postdoc, staff). Short-term appointments (undergraduate students, internships, etc.) will not involve a computer purchase unless otherwise stated as a computer can be borrowed from the group\u2019s inventory instead. It typically takes at least 2 weeks for a computer to arrive so it is recommended to prioritize placing an order either before your arrival or as soon as possible after you start.","title":"Getting a Computer "},{"location":"getting_started/buy_computer/#why-purchase-a-macbook-pro","text":"Buying a high end Macbook Pro is highly advised although not mandatory. In our experience, Macs offer the best systems for our work and having the same operating system within our group makes it easier to troubleshoot and share code. Using computers with similar hardware can also make it easier to share and pass down computer accessories and equipment. While we want to be conscious of cost and avoid wastefulness, the value of using well performing equipment that better supports our productivity, collaborative group culture, and reduces wasted time adds value which greatly outweighs the initial financial cost. If you would like to get anything other than a Macbook Pro, please consult more senior group members first. You can use the Apple website to browse details and see what is available in through the supplier, Anacapa, in Ebuy and Bearbuy. Many group members use a 13\u201d or 15\" Macbook Pro. The 13\" is powerful enough to do serious work and is lighter and smaller than the 15\" for improved mobility. However, the most powerful computing hardware is typically only available in the 15\" if you want to prioritize performance.","title":"Why Purchase a Macbook Pro?"},{"location":"getting_started/buy_computer/#making-the-purchase","text":"If you are ordering a computer before your arrival, you will need to coordinate with Alice Mueller (amuller2@lbl.gov) to handle the purchase. Please send an email with the details of your selections. Otherwise, if you are ordering a computer after you have started, you can make arrangements yourself. See purchasing for more information.","title":"Making the Purchase"},{"location":"getting_started/settling/","text":"Getting Settled: Other Helpful Information For Graduate Students New Students Moving from Another State/Country It's important that you establish California residency early in your first semester. If you don't complete the necessary tasks on time, you will be classified as a out-of-state student for the following academic year which results in higher tuition costs. Although the deadline for filing your Statement of Legal Residence (SLR) is in June, you must complete all necessary conditions for residency (getting your CA drivers license, registering your car in CA, etc) before the end of your first semester. Read the available resources and take note of the relevant deadlines for establishing California State Residency For Postdocs LBL Resources for Postdocs UC Berkeley Resources for Postdocs Postdoc Union : Postdocs have the option to join the union which aims to improve wages, hours, benefits, and other employment aspects for its members. You are welcmoe to ask other postdocs for their opnions based on their experiences with the union as well. For Affiliates LBL sends out daily emails and offers other notification options through Elements . In the past, some affiliates were not automatically signed-up for Elements announcements and missed important time sensative information contained in these updates.","title":"Getting Settled"},{"location":"getting_started/settling/#getting-settled-other-helpful-information","text":"","title":"Getting Settled: Other Helpful Information "},{"location":"getting_started/settling/#for-graduate-students","text":"","title":"For Graduate Students "},{"location":"getting_started/settling/#new-students-moving-from-another-statecountry","text":"It's important that you establish California residency early in your first semester. If you don't complete the necessary tasks on time, you will be classified as a out-of-state student for the following academic year which results in higher tuition costs. Although the deadline for filing your Statement of Legal Residence (SLR) is in June, you must complete all necessary conditions for residency (getting your CA drivers license, registering your car in CA, etc) before the end of your first semester. Read the available resources and take note of the relevant deadlines for establishing California State Residency","title":"New Students Moving from Another State/Country "},{"location":"getting_started/settling/#for-postdocs","text":"LBL Resources for Postdocs UC Berkeley Resources for Postdocs Postdoc Union : Postdocs have the option to join the union which aims to improve wages, hours, benefits, and other employment aspects for its members. You are welcmoe to ask other postdocs for their opnions based on their experiences with the union as well.","title":"For Postdocs "},{"location":"getting_started/settling/#for-affiliates","text":"LBL sends out daily emails and offers other notification options through Elements . In the past, some affiliates were not automatically signed-up for Elements announcements and missed important time sensative information contained in these updates.","title":"For Affiliates"},{"location":"getting_started/workspace/","text":"Workspace Set-Up An optimal workspace set-up is essential for making sure you can consistently contribute your best efforts. Remember, just as experimental groups invest their resources in chemicals, new lab instruments, equipment time, etc. to do their research, as computationalists, we invest in office items, software, computers, and other related tools that will make our group members more effective. So if you will use it, buy it. Don't worry so much about if you \"need\" vs. \"want\" it. Personalizing Your Workspace Please feel free to decorate your workspace with photos, posters, or other personal touches. Basic Ergonomic Workstation Accessories An effective workspace should help maximize your productivity. You will spend a great deal of time working in this space so it is worth investing in any equipment needed for you to work more efficiently and comfortably. Order any peripherals and ergonomic devices (e.g. wrist pads) such as: An external monitor : One big display screen is usually better from an ergonomic perspective than dual monitors. Note that Mac OS include features such as \"Split View\" (split your display between showing two different windows side by side) and \u201cSpaces\u201d (quickly shift between desktops displaying different sets of windows and open apps). A keyboard : The Apple Wireless Keyboard is a popular option. Some prefer mechanical keyboards. If you prefer a larger or ergonomic keyboard, you can certainly get that. A mouse/trackpad : The Apple Magic Trackpad can be a popular option for those who value consistency between their laptop and desk workstation. And it also offers additional functionality and customization built in for Mac OS (e.g. gestures). After a while you get used to doing everything on your trackpad even if you were previously very productive/accurate with a mouse on Windows. However, many group members get by just fine with a mouse (especially the Apple Magic Mouse, which has some gesture support.) Additional Recommended Computer Accessories An external hard drive (compatible with Time Machine for backing-up your computer). 4TB is a good size. Some recommend a USB-C G-Drive that also doubles as a charger. An extra computer charger A VGA and HDMI adapter dongle USB to USB-C adaptor (if you have personal electronics that rely on USB connections) Computer docking station (optional) An ethernet cable adapter dongle A presentation tool or clicker, e.g., Logitech R800 (optional) Remember to be mindful of selecting cables and devices that have compatible ports/connectors and purchasing appropriate adaptors when necessary.","title":"Workspace Set-Up"},{"location":"getting_started/workspace/#workspace-set-up","text":"An optimal workspace set-up is essential for making sure you can consistently contribute your best efforts. Remember, just as experimental groups invest their resources in chemicals, new lab instruments, equipment time, etc. to do their research, as computationalists, we invest in office items, software, computers, and other related tools that will make our group members more effective. So if you will use it, buy it. Don't worry so much about if you \"need\" vs. \"want\" it.","title":"Workspace Set-Up"},{"location":"getting_started/workspace/#personalizing-your-workspace","text":"Please feel free to decorate your workspace with photos, posters, or other personal touches.","title":"Personalizing Your Workspace"},{"location":"getting_started/workspace/#basic-ergonomic-workstation-accessories","text":"An effective workspace should help maximize your productivity. You will spend a great deal of time working in this space so it is worth investing in any equipment needed for you to work more efficiently and comfortably. Order any peripherals and ergonomic devices (e.g. wrist pads) such as: An external monitor : One big display screen is usually better from an ergonomic perspective than dual monitors. Note that Mac OS include features such as \"Split View\" (split your display between showing two different windows side by side) and \u201cSpaces\u201d (quickly shift between desktops displaying different sets of windows and open apps). A keyboard : The Apple Wireless Keyboard is a popular option. Some prefer mechanical keyboards. If you prefer a larger or ergonomic keyboard, you can certainly get that. A mouse/trackpad : The Apple Magic Trackpad can be a popular option for those who value consistency between their laptop and desk workstation. And it also offers additional functionality and customization built in for Mac OS (e.g. gestures). After a while you get used to doing everything on your trackpad even if you were previously very productive/accurate with a mouse on Windows. However, many group members get by just fine with a mouse (especially the Apple Magic Mouse, which has some gesture support.)","title":"Basic Ergonomic Workstation Accessories"},{"location":"getting_started/workspace/#additional-recommended-computer-accessories","text":"An external hard drive (compatible with Time Machine for backing-up your computer). 4TB is a good size. Some recommend a USB-C G-Drive that also doubles as a charger. An extra computer charger A VGA and HDMI adapter dongle USB to USB-C adaptor (if you have personal electronics that rely on USB connections) Computer docking station (optional) An ethernet cable adapter dongle A presentation tool or clicker, e.g., Logitech R800 (optional) Remember to be mindful of selecting cables and devices that have compatible ports/connectors and purchasing appropriate adaptors when necessary.","title":"Additional Recommended Computer Accessories"},{"location":"policies/conference_travel/","text":"Conference travel For those paid through LBNL It is important to be connected to the research community. If it is your first year in the group, you can simply attend the conferences and listen to talks. After your first year, you are expected to be presenting talks or posters at conferences. This will ensure that: you keep up to date on developments in the field you will get to know the people in the field you are broadcasting your work to the research community. Many if not most people learn about new research by hearing about it at a conference. Thus, if you want people to know about your work, you must be willing to tell people about it. You should identify conferences you\u2019d like to attend several months (usually ~4 months, perhaps ~6 months for international travel) in advance. Usually, this is around the same time that abstract deadlines are due. Once you have identified a conference you\u2019d like to attend, please take the following actions: Tell Kristin about the conference and what project you\u2019d like to present As soon as possible - submit a conference travel request form. This form is a very basic (i.e., 2 minutes to fill out) Google spreadsheet If you do not submit the travel request form several months in advance, you may not receive LBNL approval to attend. If you haven\u2019t done so already, make sure your travel profile (e.g., your frequent flier programs) are completed for the lab. E-mail esdradmin@lbl.gov if you don\u2019t have one yet. Work with Kristin to submit an abstract. You should send her the proposed abstract (with all details - title, authors, text, figures, etc.) with at least 3 days advance notice. Once you have received approval to attend the conference, please take the following steps: Make sure you register for the conference in time to receive any early registration discount (normally on one\u2019s own credit card then reimbursed later) Book a hotel (normally on one\u2019s own credit card then reimbursed later) Book a flight - please do this early to avoid last-minute flight rate spikes (normally booked in coordination with Tracee Tilman with LBNL making the booking. This works better if you identify desired flights in advance, otherwise give Tracee the preferred dates and times.) Note that if for any reason you book your own flights, you should be aware of various LBNL policies on flight booking such as preference for domestic carriers. If you are planning to combine vacation and travel, remember the lab\u2019s policy of taking only one vacation day per two work days. Note that days spent traveling to and from the conference count as work days. In terms of travel receipts and reimbursement: If you are traveling with funding through LBNL (i.e., most cases), you do not need to save receipts for meals. You will receive a per diem instead. You also do not need receipts for taxi rides under $75, although you may want to submits them anyway when you have them. You also do not need to save your actual airplane tickets for lab-purchased airfare, although again you may submit these anyway. If you are traveling with outside funding (e.g., the conference organizers are going to reimburse you), save all receipts and tickets as they may be needed for reimbursement. The proper way to request reimbursements for trips within the US is through the esdradmin site\u2019s \u201cTravel:Domestic\u201d tab . If you have trouble, you can email esdradmin@lbl.gov. As for international trips including Canada, you should get in touch with the ESDR admin person that you work with. That person will provide you a corresponding form and help you through the (more complex) process of international-travel reimbursement. Pro tip: If you want to see the status of your conference requests, log in to this sheet with your LBNL account You can filter the sheet to your requests by right-clicking on the name column and choosing the filter option. You should look for the \u201c(ADMINS ONLY) Approval status\u201d column in order to check your status. For those paid through UC Berkeley More details coming to this documentation soon.","title":"Conference Travel"},{"location":"policies/conference_travel/#conference-travel","text":"","title":"Conference travel "},{"location":"policies/conference_travel/#for-those-paid-through-lbnl","text":"It is important to be connected to the research community. If it is your first year in the group, you can simply attend the conferences and listen to talks. After your first year, you are expected to be presenting talks or posters at conferences. This will ensure that: you keep up to date on developments in the field you will get to know the people in the field you are broadcasting your work to the research community. Many if not most people learn about new research by hearing about it at a conference. Thus, if you want people to know about your work, you must be willing to tell people about it. You should identify conferences you\u2019d like to attend several months (usually ~4 months, perhaps ~6 months for international travel) in advance. Usually, this is around the same time that abstract deadlines are due. Once you have identified a conference you\u2019d like to attend, please take the following actions: Tell Kristin about the conference and what project you\u2019d like to present As soon as possible - submit a conference travel request form. This form is a very basic (i.e., 2 minutes to fill out) Google spreadsheet If you do not submit the travel request form several months in advance, you may not receive LBNL approval to attend. If you haven\u2019t done so already, make sure your travel profile (e.g., your frequent flier programs) are completed for the lab. E-mail esdradmin@lbl.gov if you don\u2019t have one yet. Work with Kristin to submit an abstract. You should send her the proposed abstract (with all details - title, authors, text, figures, etc.) with at least 3 days advance notice. Once you have received approval to attend the conference, please take the following steps: Make sure you register for the conference in time to receive any early registration discount (normally on one\u2019s own credit card then reimbursed later) Book a hotel (normally on one\u2019s own credit card then reimbursed later) Book a flight - please do this early to avoid last-minute flight rate spikes (normally booked in coordination with Tracee Tilman with LBNL making the booking. This works better if you identify desired flights in advance, otherwise give Tracee the preferred dates and times.) Note that if for any reason you book your own flights, you should be aware of various LBNL policies on flight booking such as preference for domestic carriers. If you are planning to combine vacation and travel, remember the lab\u2019s policy of taking only one vacation day per two work days. Note that days spent traveling to and from the conference count as work days. In terms of travel receipts and reimbursement: If you are traveling with funding through LBNL (i.e., most cases), you do not need to save receipts for meals. You will receive a per diem instead. You also do not need receipts for taxi rides under $75, although you may want to submits them anyway when you have them. You also do not need to save your actual airplane tickets for lab-purchased airfare, although again you may submit these anyway. If you are traveling with outside funding (e.g., the conference organizers are going to reimburse you), save all receipts and tickets as they may be needed for reimbursement. The proper way to request reimbursements for trips within the US is through the esdradmin site\u2019s \u201cTravel:Domestic\u201d tab . If you have trouble, you can email esdradmin@lbl.gov. As for international trips including Canada, you should get in touch with the ESDR admin person that you work with. That person will provide you a corresponding form and help you through the (more complex) process of international-travel reimbursement. Pro tip: If you want to see the status of your conference requests, log in to this sheet with your LBNL account You can filter the sheet to your requests by right-clicking on the name column and choosing the filter option. You should look for the \u201c(ADMINS ONLY) Approval status\u201d column in order to check your status.","title":"For those paid through LBNL"},{"location":"policies/conference_travel/#for-those-paid-through-uc-berkeley","text":"More details coming to this documentation soon.","title":"For those paid through UC Berkeley"},{"location":"policies/leave_time/","text":"Leave Time Reporting You should coordinate the specific days of vacation and personal time off with Kristin, especially for an extended absence. Graduate Students Graduate students receive # PTO days, along with all holidays observed by LBNL. LBL Postdocs Postdocs will receive a set number of vacation / personal time off (PTO) days that will be outlined in your hiring package. For union postdocs, the union has currently negotiated 24 PTO days per year along with other benefits. Berkeley Postdocs As a new postdoc, you should submit a Postdoc time sheet each month to the Payroll team at 197M Cory Hall, ersopayroll@erso.berkeley.edu. This time sheet is used only to record the Personal Time Off and Sick Leave that you use. According to UC Postdoc policy, you are granted 24 days of Personal Time Off and 12 days of Sick Leave per 12-month appointment.","title":"Leave Time"},{"location":"policies/leave_time/#leave-time-reporting","text":"You should coordinate the specific days of vacation and personal time off with Kristin, especially for an extended absence.","title":"Leave Time Reporting"},{"location":"policies/leave_time/#graduate-students","text":"Graduate students receive # PTO days, along with all holidays observed by LBNL.","title":"Graduate Students "},{"location":"policies/leave_time/#lbl-postdocs","text":"Postdocs will receive a set number of vacation / personal time off (PTO) days that will be outlined in your hiring package. For union postdocs, the union has currently negotiated 24 PTO days per year along with other benefits.","title":"LBL Postdocs "},{"location":"policies/leave_time/#berkeley-postdocs","text":"As a new postdoc, you should submit a Postdoc time sheet each month to the Payroll team at 197M Cory Hall, ersopayroll@erso.berkeley.edu. This time sheet is used only to record the Personal Time Off and Sick Leave that you use. According to UC Postdoc policy, you are granted 24 days of Personal Time Off and 12 days of Sick Leave per 12-month appointment.","title":"Berkeley Postdocs "},{"location":"policies/meetings/","text":"Mentoring and Individual Meetings Individual meetings with Kristin are the best venue for seeking detailed feedback on your projects and giving updates It is up to individual group members to schedule these meetings as often as needed. First and second year students are particularly encouraged to schedule regular meetings with Kristin. You should be able to get a meeting as long as you schedule it at least 1 week in advance. If you are having trouble getting a meeting within that timeframe, email Kristin. Mentoring in the Persson group goes many directions (Kristin \u2194\ufe0e students, staff \u2194\ufe0e postdocs, students \u2194\ufe0e staff, students \u2194\ufe0e students etc.) and you are encouraged to cultivate collaborative and mentoring relationships with colleagues throughout the group. Mentoring and supporting one another is a core part of our responsibilities as group members. Mentoring is not hierarchical. More senior colleagues can be valuable sources of feedback and advice, but you are not obligated to listen to them! Ultimate decision making rests with Kristin, other feedback is advisory only. Everyone in the group should feel that they have at least two other group members (in addition to Kristin) they can talk to about their work. If you do not feel this way, please contact Kristin, or any postdoc or staff member. In addition, if you feel like you are not getting mentorship you need for a specific aspect of your project within the group, let us know and we can help you find someone to talk with, whether inside or outside the group. Big group meeting and subgroup meetings are designed to provide other avenues for feedback and to foster mentoring and collaboration at all levels of the Persson group. \"Big\" Group Meeting Big group presentations are an opportunity for one group member (student, postdoc, or staff) to give a longer, seminar-style presentation and for other group members to see what colleagues they may not interact with frequently are working on. All group members are expected to attend the big group meeting. If you have a conflict, please do your best to let Kristin know in advance via email. Big group meetings occur bi-weekly and Kristin always attends. The group meeting organizer will designate a group member to present at each meeting on a rotating schedule . Given the size of our group, each person can expect to present at big group meeting about once per year. At the beginning or the end of each meeting, we will have 5-10 minutes for announcements and informal discussion. As long as working remotely is the norm, this should include a \u201cself care moment\u201d. This may also include 5-10 minutes on a DEI topic Subgroup Meetings The primary purpose of subgroups is to provide a forum for discussing best practices and problem-solving with your peers All discussion within and outside subgroups should adhere to the standards of conduct in the Persson Group Agreements Members are not expected to make formal presentations at subgroup meetings, but are expected to periodically share what they are working on (including what they may be stuck on or struggling with) and ask questions of their peers. Subgroup members that want to speak at a meeting are encouraged to contact their subgroup organizer about 3 days in advance of the meeting to claim time on the agenda. Subgroup organizers are encouraged to send email reminders containing a brief agenda or list of speakers 1-2 days in advance of the meeting. The subgroup organizer is also responsible for moderating the meeting to make sure everyone who asked to speak gets time to do so. Members are expected to make an effort to listen, ask questions, and provide feedback to their peers, even if their peers\u2019 projects are not directly related to their work. Learning to communicate across gaps in one\u2019s own knowledge is an important skill! All group members are expected to regularly attend at least one research subgroup meeting every 2 weeks. You may drop in and out of different subgroups on different weeks as you see fit, as long as you are meeting attendance criteria In addition to research subgroups, there are also several optional subgroups that you are welcome and encouraged to attend. These do not count toward the attendance requirement, however. Subgroup meetings occur bi-weekly and will be posted on the perssongroup calendar Every subgroup will be attended by at least two postdocs or staff members Kristin\u2019s attendance is not mandatory, but will be indicated ahead of time by calendar invite As long as working remotely is the norm, subgroup meetings should include ~15 min of unstructured social time at the start or end as a way of maintaining our sense of community and fighting isolation If subgroups get too large (regularly attended by more than ~8-10 people), they will be split into smaller subgroups Research subgroups are organized by theme according to different methods. As of November 2020, we have the following subgroups: Research Subgroups Periodic Bulk DFT (i.e., VASP) -- Jimmy Shen Periodic Slab DFT (i.e., VASP surfaces) -- Ruoxi Yang Molecular DFT (i.e., Gaussian/QChem) -- TBD MD and Electrolytes -- Julian Self Machine Learning -- Mingjian Wen Reaction networks / SEI -- Sam Blau Non-Research Subgroups DEI Subgroup -- TBD Learn about and discuss topics related to Diversity, Equity, and Inclusion, especially as they relate to academia and science Journal Club -- TBD *Discussion of interesting papers The subgroup organizer will designate one member to pick a paper and lead the discussion in each meeting on a rotating basis Coding Subgroup -- Patrick Huck Discussion of pymatgen/atomate/custodian/etc. development (recent and anticipated PRs, new features, etc.) Coding best practices, tips, and tricks Examples given in subgroup may be developed into documentation Seminar Subgroup -- Ryan Kingsbury *Opportunity for one group member to give a 20-min conference style talk on a topic of their choice, and solicit feedback on presentation skills The subgroup organizer will coordinate a schedule of presenters based on requests Speakers should prepare a 15-20 min presentation and indicate what kind of feedback would be useful to them (e.g. content, delivery, graphics, general discussion)","title":"Meetings"},{"location":"policies/meetings/#mentoring-and-individual-meetings","text":"Individual meetings with Kristin are the best venue for seeking detailed feedback on your projects and giving updates It is up to individual group members to schedule these meetings as often as needed. First and second year students are particularly encouraged to schedule regular meetings with Kristin. You should be able to get a meeting as long as you schedule it at least 1 week in advance. If you are having trouble getting a meeting within that timeframe, email Kristin. Mentoring in the Persson group goes many directions (Kristin \u2194\ufe0e students, staff \u2194\ufe0e postdocs, students \u2194\ufe0e staff, students \u2194\ufe0e students etc.) and you are encouraged to cultivate collaborative and mentoring relationships with colleagues throughout the group. Mentoring and supporting one another is a core part of our responsibilities as group members. Mentoring is not hierarchical. More senior colleagues can be valuable sources of feedback and advice, but you are not obligated to listen to them! Ultimate decision making rests with Kristin, other feedback is advisory only. Everyone in the group should feel that they have at least two other group members (in addition to Kristin) they can talk to about their work. If you do not feel this way, please contact Kristin, or any postdoc or staff member. In addition, if you feel like you are not getting mentorship you need for a specific aspect of your project within the group, let us know and we can help you find someone to talk with, whether inside or outside the group. Big group meeting and subgroup meetings are designed to provide other avenues for feedback and to foster mentoring and collaboration at all levels of the Persson group.","title":"Mentoring and Individual Meetings "},{"location":"policies/meetings/#big-group-meeting","text":"Big group presentations are an opportunity for one group member (student, postdoc, or staff) to give a longer, seminar-style presentation and for other group members to see what colleagues they may not interact with frequently are working on. All group members are expected to attend the big group meeting. If you have a conflict, please do your best to let Kristin know in advance via email. Big group meetings occur bi-weekly and Kristin always attends. The group meeting organizer will designate a group member to present at each meeting on a rotating schedule . Given the size of our group, each person can expect to present at big group meeting about once per year. At the beginning or the end of each meeting, we will have 5-10 minutes for announcements and informal discussion. As long as working remotely is the norm, this should include a \u201cself care moment\u201d. This may also include 5-10 minutes on a DEI topic","title":"\"Big\" Group Meeting "},{"location":"policies/meetings/#subgroup-meetings","text":"The primary purpose of subgroups is to provide a forum for discussing best practices and problem-solving with your peers All discussion within and outside subgroups should adhere to the standards of conduct in the Persson Group Agreements Members are not expected to make formal presentations at subgroup meetings, but are expected to periodically share what they are working on (including what they may be stuck on or struggling with) and ask questions of their peers. Subgroup members that want to speak at a meeting are encouraged to contact their subgroup organizer about 3 days in advance of the meeting to claim time on the agenda. Subgroup organizers are encouraged to send email reminders containing a brief agenda or list of speakers 1-2 days in advance of the meeting. The subgroup organizer is also responsible for moderating the meeting to make sure everyone who asked to speak gets time to do so. Members are expected to make an effort to listen, ask questions, and provide feedback to their peers, even if their peers\u2019 projects are not directly related to their work. Learning to communicate across gaps in one\u2019s own knowledge is an important skill! All group members are expected to regularly attend at least one research subgroup meeting every 2 weeks. You may drop in and out of different subgroups on different weeks as you see fit, as long as you are meeting attendance criteria In addition to research subgroups, there are also several optional subgroups that you are welcome and encouraged to attend. These do not count toward the attendance requirement, however. Subgroup meetings occur bi-weekly and will be posted on the perssongroup calendar Every subgroup will be attended by at least two postdocs or staff members Kristin\u2019s attendance is not mandatory, but will be indicated ahead of time by calendar invite As long as working remotely is the norm, subgroup meetings should include ~15 min of unstructured social time at the start or end as a way of maintaining our sense of community and fighting isolation If subgroups get too large (regularly attended by more than ~8-10 people), they will be split into smaller subgroups Research subgroups are organized by theme according to different methods. As of November 2020, we have the following subgroups:","title":"Subgroup Meetings "},{"location":"policies/meetings/#research-subgroups","text":"Periodic Bulk DFT (i.e., VASP) -- Jimmy Shen Periodic Slab DFT (i.e., VASP surfaces) -- Ruoxi Yang Molecular DFT (i.e., Gaussian/QChem) -- TBD MD and Electrolytes -- Julian Self Machine Learning -- Mingjian Wen Reaction networks / SEI -- Sam Blau","title":"Research Subgroups "},{"location":"policies/meetings/#non-research-subgroups","text":"DEI Subgroup -- TBD Learn about and discuss topics related to Diversity, Equity, and Inclusion, especially as they relate to academia and science Journal Club -- TBD *Discussion of interesting papers The subgroup organizer will designate one member to pick a paper and lead the discussion in each meeting on a rotating basis Coding Subgroup -- Patrick Huck Discussion of pymatgen/atomate/custodian/etc. development (recent and anticipated PRs, new features, etc.) Coding best practices, tips, and tricks Examples given in subgroup may be developed into documentation Seminar Subgroup -- Ryan Kingsbury *Opportunity for one group member to give a 20-min conference style talk on a topic of their choice, and solicit feedback on presentation skills The subgroup organizer will coordinate a schedule of presenters based on requests Speakers should prepare a 15-20 min presentation and indicate what kind of feedback would be useful to them (e.g. content, delivery, graphics, general discussion)","title":"Non-Research Subgroups "},{"location":"policies/purchasing/","text":"Purchasing LBNL Funded Members Use LBNL Ebuy (not Ebay) wherever possible - you need to be on the lab network (onsite via an ethernet cable) or be connected via the VPN Use Amazon, etc. to buy various components if not available via EBuy The laptop is government property; you are expected to return it to the group when you are done working at LBNL. Note that Mac computers make it very simple to transfer everything over to your next computer. You are free to take your laptop home, on trips, etc., unless you are an intern in which case other restrictions may apply from the internship program. The lab receives your computer and tags it before sending it over to you. You must back up your computer very regularly (at least once per week, ideally continuously). This is simple using the Time Machine app. Just plug your backup drive into your monitor so when you connect to your monitor, you also back up. If there are (for some reason) errors in backing up, fix that issue immediately. There are zero excuses for not doing this. Making the purchase Provide all the details of your selections in an email and send to Alice Mueller. If all looks ok, she will give you a project and activity ID. Go to eBuy, and for items available there, add them to your cart and submit the requisition with the project and activity ID. Ask Alice Mueller about which SAS approver to list if you are unsure (the SAS approver can vary by project and activity ID). For items not available on eBuy, contact esdradmin@lbl.gov (and cc Kristin and Alice) to obtain a procurement form. Fill it out with item details (Vendor, website, price, etc.) and send it back. If you select the overnight shipping option (ask Kristin about this and the related extra costs) most parts, except the computer, will arrive within a week to 10 days. The computer needs to be tagged by the lab, so with overnight shipping, it should arrive within 2 weeks. Ideally, you will select your computer well before arriving at the lab and won\u2019t need overnight shipping. UCB Funded Members Use BearBuy More details coming to this documentation soon.","title":"Purchasing"},{"location":"policies/purchasing/#purchasing","text":"","title":"Purchasing"},{"location":"policies/purchasing/#lbnl-funded-members","text":"Use LBNL Ebuy (not Ebay) wherever possible - you need to be on the lab network (onsite via an ethernet cable) or be connected via the VPN Use Amazon, etc. to buy various components if not available via EBuy The laptop is government property; you are expected to return it to the group when you are done working at LBNL. Note that Mac computers make it very simple to transfer everything over to your next computer. You are free to take your laptop home, on trips, etc., unless you are an intern in which case other restrictions may apply from the internship program. The lab receives your computer and tags it before sending it over to you. You must back up your computer very regularly (at least once per week, ideally continuously). This is simple using the Time Machine app. Just plug your backup drive into your monitor so when you connect to your monitor, you also back up. If there are (for some reason) errors in backing up, fix that issue immediately. There are zero excuses for not doing this.","title":"LBNL Funded Members"},{"location":"policies/purchasing/#making-the-purchase","text":"Provide all the details of your selections in an email and send to Alice Mueller. If all looks ok, she will give you a project and activity ID. Go to eBuy, and for items available there, add them to your cart and submit the requisition with the project and activity ID. Ask Alice Mueller about which SAS approver to list if you are unsure (the SAS approver can vary by project and activity ID). For items not available on eBuy, contact esdradmin@lbl.gov (and cc Kristin and Alice) to obtain a procurement form. Fill it out with item details (Vendor, website, price, etc.) and send it back. If you select the overnight shipping option (ask Kristin about this and the related extra costs) most parts, except the computer, will arrive within a week to 10 days. The computer needs to be tagged by the lab, so with overnight shipping, it should arrive within 2 weeks. Ideally, you will select your computer well before arriving at the lab and won\u2019t need overnight shipping.","title":"Making the purchase"},{"location":"policies/purchasing/#ucb-funded-members","text":"Use BearBuy More details coming to this documentation soon.","title":"UCB Funded Members"}]}